{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "double_well_minimal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPA1b97k6VA6t4FbwKBPG9r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zierenberg/machine_learning_muca/blob/MW/double_well_minimal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkbmlc-_ZtSA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "ed13852e-02b1-4a74-a5e5-107560b209ba"
      },
      "source": [
        "!git clone https://github.com/zierenberg/machine_learning_muca.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'machine_learning_muca'...\n",
            "remote: Enumerating objects: 78, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 78 (delta 16), reused 40 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (78/78), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WZMDn37Mx_W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "00e29268-aedb-48ae-fd00-d188ba7d67ba"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import sys"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi2psdzKakPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DoubleWell(object):\n",
        "\n",
        "    params_default = {'a4' : 1.0,\n",
        "                      'a2' : 6.0,\n",
        "                      'a1' : 1.0,\n",
        "                      'k' : 1.0,\n",
        "                      'dim' : 1}\n",
        "\n",
        "    def __init__(self, params=None):\n",
        "        # set parameters\n",
        "        if params is None:\n",
        "            params = self.__class__.params_default\n",
        "        self.params = params\n",
        "\n",
        "        # useful variables\n",
        "        self.dim = self.params['dim']\n",
        "\n",
        "\n",
        "    def energy(self, x):\n",
        "        dimer_energy =self.params['a4'] * x[:, 0] ** 4 - self.params['a2'] * x[:, 0] ** 2 + self.params['a1'] * x[:, 0]\n",
        "        oscillator_energy = 0.0\n",
        "        if self.dim == 2:\n",
        "            oscillator_energy = (self.params['k'] / 2.0) * x[:, 1] ** 2\n",
        "        if self.dim > 2:\n",
        "            oscillator_energy = np.sum((self.params['k'] / 2.0) * x[:, 1:] ** 2, axis=1)\n",
        "        return  dimer_energy + oscillator_energy\n",
        "\n",
        "    def energy_tf(self, x):\n",
        "        dimer_energy =self.params['a4'] * x[:, 0] ** 4 - self.params['a2'] * x[:, 0] ** 2 + self.params['a1'] * x[:, 0]\n",
        "        oscillator_energy = 0.0\n",
        "        if self.dim == 2:\n",
        "            oscillator_energy = (self.params['k'] / 2.0) * x[:, 1] ** 2\n",
        "        if self.dim > 2:\n",
        "            oscillator_energy = tf.reduce_sum(input_tensor=(self.params['k'] / 2.0) * x[:, 1:] ** 2, axis=1)\n",
        "        return  dimer_energy + oscillator_energy\n",
        "\n",
        "    def plot_dimer_energy(self, axis=None, temperature=1.0):\n",
        "        \"\"\" Plots the dimer energy to the standard figure \"\"\"\n",
        "        x_grid = np.linspace(-3, 3, num=200)\n",
        "        if self.dim == 1:\n",
        "            X = x_grid[:, None]\n",
        "        else:\n",
        "            X = np.hstack([x_grid[:, None], np.zeros((x_grid.size, self.dim - 1))])\n",
        "        energies = self.energy(X) / temperature\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "        if axis is None:\n",
        "            axis = plt.gca()\n",
        "        #plt.figure(figsize=(5, 4))\n",
        "        axis.plot(x_grid, energies, linewidth=3, color='black')\n",
        "        axis.set_xlabel('x / a.u.')\n",
        "        axis.set_ylabel('Energy / kT')\n",
        "        axis.set_ylim(energies.min() - 2.0, energies[int(energies.size / 2)] + 2.0)\n",
        "\n",
        "        return x_grid, energies"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7eITfCBbVTI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "c6b77f99-fe59-4558-cfa1-bcb784b5372d"
      },
      "source": [
        "params = DoubleWell.params_default.copy()\n",
        "params['dim'] =2\n",
        "double_well = DoubleWell(params=params)\n",
        "plt.figure(figsize=(5,5))\n",
        "double_well.plot_dimer_energy();"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPSIKMgabbiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MetropolisGauss(object):\n",
        "\n",
        "    def __init__(self, model, x0, temperature=1.0, noise=0.1,\n",
        "                 burnin=0, stride=1, nwalkers=1, mapper=None):\n",
        "        \"\"\" Metropolis Monte-Carlo Simulation with Gaussian Proposal Steps\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model : Energy model\n",
        "            Energy model object, must provide the function energy(x)\n",
        "        x0 : [array]\n",
        "            Initial configuration\n",
        "        noise : float\n",
        "            Noise intensity, standard deviation of Gaussian proposal step\n",
        "        temperatures : float or array\n",
        "            Temperature. By default (1.0) the energy is interpreted in reduced units.\n",
        "            When given an array, its length must correspond to nwalkers, then the walkers\n",
        "            are simulated at different temperatures.\n",
        "        burnin : int\n",
        "            Number of burn-in steps that will not be saved\n",
        "        stride : int\n",
        "            Every so many steps will be saved\n",
        "        nwalkers : int\n",
        "            Number of parallel walkers\n",
        "        mapper : Mapper object\n",
        "            Object with function map(X), e.g. to remove permutation.\n",
        "            If given will be applied to each accepted configuration.\n",
        "\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.noise = noise\n",
        "        self.temperature = temperature\n",
        "        self.burnin = burnin\n",
        "        self.stride = stride\n",
        "        self.nwalkers = nwalkers\n",
        "        if mapper is None:\n",
        "            class DummyMapper(object):\n",
        "                def map(self, X):\n",
        "                    return X\n",
        "            mapper = DummyMapper()\n",
        "        self.mapper = mapper\n",
        "        self.reset(x0)\n",
        "\n",
        "    def _proposal_step(self):\n",
        "        # proposal step\n",
        "        self.x_prop = self.x + self.noise*np.random.randn(self.x.shape[0], self.x.shape[1])\n",
        "        self.x_prop = self.mapper.map(self.x_prop)\n",
        "        self.E_prop = self.model.energy(self.x_prop)\n",
        "\n",
        "    def _acceptance_step(self):\n",
        "        # acceptance step\n",
        "        acc = -np.log(np.random.rand()) > (self.E_prop - self.E) / self.temperature\n",
        "        self.x = np.where(acc[:, None], self.x_prop, self.x)\n",
        "        self.E = np.where(acc, self.E_prop, self.E)\n",
        "\n",
        "    def reset(self, x0):\n",
        "        # counters\n",
        "        self.step = 0\n",
        "        self.traj_ = []\n",
        "        self.etraj_ = []\n",
        "\n",
        "        # initial configuration\n",
        "        self.x = np.tile(x0, (self.nwalkers, 1))\n",
        "        self.x = self.mapper.map(self.x)\n",
        "        self.E = self.model.energy(self.x)\n",
        "\n",
        "        # save first frame if no burnin\n",
        "        if self.burnin == 0:\n",
        "            self.traj_.append(self.x)\n",
        "            self.etraj_.append(self.E)\n",
        "\n",
        "    @property\n",
        "    def trajs(self):\n",
        "        \"\"\" Returns a list of trajectories, one trajectory for each walker \"\"\"\n",
        "        T = np.array(self.traj_).astype(np.float32)\n",
        "        return [T[:, i, :] for i in range(T.shape[1])]\n",
        "\n",
        "    @property\n",
        "    def traj(self):\n",
        "        return self.trajs[0]\n",
        "\n",
        "    @property\n",
        "    def etrajs(self):\n",
        "        \"\"\" Returns a list of energy trajectories, one trajectory for each walker \"\"\"\n",
        "        E = np.array(self.etraj_)\n",
        "        return [E[:, i] for i in range(E.shape[1])]\n",
        "\n",
        "    @property\n",
        "    def etraj(self):\n",
        "        return self.etrajs[0]\n",
        "\n",
        "    def run(self, nsteps=1, verbose=0):\n",
        "        for i in range(nsteps):\n",
        "            self._proposal_step()\n",
        "            self._acceptance_step()\n",
        "            self.step += 1\n",
        "            if verbose > 0 and i % verbose == 0:\n",
        "                print('Step', i, '/', nsteps)\n",
        "            if self.step > self.burnin and self.step % self.stride == 0:\n",
        "                self.traj_.append(self.x)\n",
        "                self.etraj_.append(self.E)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raqT2XoaeHgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nsteps = 10000\n",
        "x0_left = np.array([[-1.8, 0.0]])\n",
        "x0_right = np.array([[1.8, 0.0]])\n",
        "\n",
        "sampler = MetropolisGauss(double_well, x0_left, noise=0.1, stride=10)\n",
        "sampler.run(nsteps)\n",
        "traj_left = sampler.traj.copy()\n",
        "\n",
        "sampler.reset(x0_left)\n",
        "sampler.run(nsteps)\n",
        "traj_left_val = sampler.traj.copy()\n",
        "\n",
        "sampler.reset(x0_right)\n",
        "sampler.run(nsteps)\n",
        "traj_right = sampler.traj.copy()\n",
        "\n",
        "sampler.reset(x0_right)\n",
        "sampler.run(nsteps)\n",
        "traj_right_val = sampler.traj.copy()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1X_c42pJSwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "621e4b8f-49d7-4836-8f77-ddb951b4f44b"
      },
      "source": [
        "plt.figure(figsize=(9, 4))\n",
        "ax1 = plt.subplot2grid((1, 3), (0, 0), colspan=2)\n",
        "ax2 = plt.subplot2grid((1, 3), (0, 2))\n",
        "ax1.plot(traj_left[:, 0], color='blue', alpha=0.7)\n",
        "ax1.plot(traj_right[:, 0], color='red', alpha=0.7)\n",
        "ax1.set_xlim(0, 1000)\n",
        "ax1.set_ylim(-2.5, 2.5)\n",
        "ax1.set_xlabel('Time / steps')\n",
        "ax1.set_ylabel('$x_1$ / a.u.')\n",
        "ax2.hist(traj_left[:, 0], 30, orientation='horizontal', histtype='stepfilled', color='blue', alpha=0.2);\n",
        "ax2.hist(traj_left[:, 0], 30, orientation='horizontal', histtype='step', color='blue', linewidth=2);\n",
        "ax2.hist(traj_right[:, 0], 30, orientation='horizontal', histtype='stepfilled', color='red', alpha=0.2);\n",
        "ax2.hist(traj_right[:, 0], 30, orientation='horizontal', histtype='step', color='red', linewidth=2);\n",
        "ax2.set_xticks([])\n",
        "ax2.set_yticks([])\n",
        "ax2.set_ylim(-2.5, 2.5)\n",
        "ax2.set_xlabel('Probability')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Probability')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWZgO2a9SpUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.vstack([traj_left, traj_right])\n",
        "xval = np.vstack([traj_left_val, traj_right_val])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvlEvEwARab-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# util.py\n",
        "\n",
        "def ensure_traj(X):\n",
        "    if np.ndim(X) == 2:\n",
        "        return X\n",
        "    if np.ndim(X) == 1:\n",
        "        return np.array([X])\n",
        "    raise ValueError('Incompatible array with shape: ', np.shape(X))\n",
        "\n",
        "def connect(input_layer, layers):\n",
        "    \"\"\" Connect the given sequence of layers and returns output layer\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_layer : keras layer\n",
        "        Input layer\n",
        "    layers : list of keras layers\n",
        "        Layers to be connected sequentially\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    output_layer : kears layer\n",
        "        Output Layer\n",
        "\n",
        "    \"\"\"\n",
        "    layer = input_layer\n",
        "    for l in layers:\n",
        "        layer = l(layer)\n",
        "    return layer\n",
        "\n",
        "def linlogcut(x, a=0, b=1000, tf=False):\n",
        "    \"\"\" Function which is linear until a, logarithmic until b and then constant.\n",
        "\n",
        "    y = x                  x <= a\n",
        "    y = a + log(x-a)   a < x < b\n",
        "    y = a + log(b-a)   b < x\n",
        "\n",
        "    \"\"\"\n",
        "    if tf:\n",
        "        return _linlogcut_tf(x, a=a, b=b)\n",
        "    else:\n",
        "        return _linlogcut_np(x, a=a, b=b)\n",
        "\n",
        "def _linlogcut_tf(x, a=0, b=1000):\n",
        "    # cutoff x after b - this should also cutoff infinities\n",
        "    x = tf.compat.v1.where(x < b, x, b * tf.ones(tf.shape(input=x)))\n",
        "    # log after a\n",
        "    y = a + tf.compat.v1.where(x < a, x - a, tf.math.log(x - a + 1))\n",
        "    # make sure everything is finite\n",
        "    y = tf.compat.v1.where(tf.math.is_finite(y), y, b * tf.ones(tf.shape(input=y)))\n",
        "    return y\n",
        "\n",
        "def _linlogcut_tf_constantclip(x, a=0, b=1000):\n",
        "    # cutoff x after b - this should also cutoff infinities\n",
        "    x = tf.compat.v1.where(x < b, x, x / tf.maximum(1., tf.stop_gradient(x) / b))\n",
        "    # cutoff x after b - this should also cutoff infinities\n",
        "    #x = tf.where(x < b, x, b * tf.ones(tf.shape(x)))\n",
        "    # log after a\n",
        "    y = a + tf.compat.v1.where(x < a, x - a, tf.math.log(x - a + 1))\n",
        "    # make sure everything is finite\n",
        "    y = tf.compat.v1.where(tf.math.is_finite(y), y, b * tf.ones(tf.shape(input=y)))\n",
        "    return y\n",
        "\n",
        "def _linlogcut_np(x, a=0, b=1000):\n",
        "    raise NotImplementedError('Numpy version not yet implemented.')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUbt_dM0P2Yz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# layers_basic.py\n",
        "\n",
        "import numbers\n",
        "\n",
        "def nonlinear_transform(output_size, nlayers=3, nhidden=100, activation='relu', init_outputs=None, **args):\n",
        "    \"\"\" Generic dense trainable nonlinear transform\n",
        "\n",
        "    Returns the layers of a dense feedforward network with nlayers-1 hidden layers with nhidden neurons\n",
        "    and the specified activation functions. The last layer is linear in order to access the full real\n",
        "    number range and has output_size output neurons.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    output_size : int\n",
        "        number of output neurons\n",
        "    nlayers : int\n",
        "        number of layers, including the linear output layer. nlayers=3 means two hidden layers with\n",
        "        nonlinear activation and one linear output layer.\n",
        "    nhidden : int\n",
        "        number of neurons in each hidden layer, either a number or an array of length nlayers-1\n",
        "        to specify the width of each hidden layer\n",
        "    activation : str\n",
        "        nonlinear activation function in hidden layers\n",
        "    init_outputs : None or float or array\n",
        "        None means default initialization for the output layer, otherwise it is currently initialized with 0\n",
        "    **args : kwargs\n",
        "        Additional keyword arguments passed to the layer\n",
        "\n",
        "    \"\"\"\n",
        "    if isinstance(nhidden, numbers.Integral):\n",
        "        nhidden = nhidden * np.ones(nlayers-1, dtype=int)\n",
        "    else:\n",
        "        nhidden = np.array(nhidden)\n",
        "        if nhidden.size != nlayers-1:\n",
        "            raise ValueError('Illegal size of nhidden. Expecting 1d array with nlayers-1 elements')\n",
        "    M = [keras.layers.Dense(nh, activation=activation, **args) for nh in nhidden]\n",
        "    if init_outputs is None:\n",
        "        final_layer = keras.layers.Dense(output_size, activation='linear', **args)\n",
        "    else:\n",
        "        argscopy = copy.deepcopy(args)\n",
        "        argscopy['kernel_initializer'] = keras.initializers.Zeros()\n",
        "        argscopy['bias_initializer'] = keras.initializers.Constant(init_outputs)\n",
        "        final_layer = keras.layers.Dense(output_size, activation='linear', **argscopy)\n",
        "                                         #kernel_initializer=keras.initializers.Zeros(),\n",
        "                                         #bias_initializer=keras.initializers.Constant(init_outputs))\n",
        "    M += [final_layer]\n",
        "\n",
        "    return M\n",
        "\n",
        "\n",
        "class ResampleLayer(keras.engine.Layer):\n",
        "    \"\"\"\n",
        "    Receives as inputs latent space encodings z and normal noise w. Transforms w to\n",
        "    Match the mean and the standard deviations of z.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, **kwargs):\n",
        "        self.dim = dim\n",
        "        super(ResampleLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, x):\n",
        "        # split input into latent and noise variables\n",
        "        z = x[:, :self.dim]\n",
        "        w = x[:, self.dim:]\n",
        "        #z, w = x\n",
        "        # mean\n",
        "        mean = keras.backend.mean(z, axis=0)\n",
        "        # covariance matrix\n",
        "        batchsize = keras.backend.shape(z)[0]\n",
        "        cov = keras.backend.dot(keras.backend.transpose(z), z) / keras.backend.cast(batchsize, np.float32)\n",
        "        # standard deviations\n",
        "        std = tf.sqrt(tf.linalg.tensor_diag_part(cov))\n",
        "        # transform w and return\n",
        "        wtrans = tf.reshape(mean, (1, self.dim)) + w * tf.reshape(std, (1, self.dim))\n",
        "        return wtrans\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], self.dim\n",
        "\n",
        "\n",
        "class IndexLayer(keras.engine.Layer):\n",
        "    def __init__(self, indices, **kwargs):\n",
        "        \"\"\" Returns [:, indices].\n",
        "        \"\"\"\n",
        "        self.indices = indices\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, x):\n",
        "        # split input\n",
        "        return tf.gather(x, self.indices, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], self.indices.size"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FhbACMiLwrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# invertible_layers.py\n",
        "\n",
        "def split_merge_indices(ndim, nchannels=2, channels=None):\n",
        "    if channels is None:\n",
        "        channels = np.tile(np.arange(nchannels), int(ndim/nchannels)+1)[:ndim]\n",
        "    else:\n",
        "        channels = np.array(channels)\n",
        "        nchannels = np.max(channels) + 1\n",
        "    indices_split = []\n",
        "    idx = np.arange(ndim)\n",
        "    for c in range(nchannels):\n",
        "        isplit = np.where(channels == c)[0]\n",
        "        indices_split.append(isplit)\n",
        "    indices_merge = np.concatenate(indices_split).argsort()\n",
        "    return channels, indices_split, indices_merge\n",
        "\n",
        "class Permute(object):\n",
        "    def __init__(self, ndim, order=None):\n",
        "        \"\"\" Permutes dimensions\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        order : None or array\n",
        "            If None, a random permutation will be chosen.\n",
        "            Otherwise, specify the new order of dimensions for x -> z.\n",
        "\n",
        "        \"\"\"\n",
        "        self.ndim = ndim\n",
        "        if order is None:\n",
        "            order = np.random.choice(ndim, ndim, replace=False)\n",
        "        self.order = order\n",
        "        self.reverse = np.argsort(order)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, D):\n",
        "        ndim = D['ndim']\n",
        "        order = D['order']\n",
        "        return cls(ndim, order=order)\n",
        "\n",
        "    def to_dict(self):\n",
        "        D = {}\n",
        "        D['ndim'] = self.ndim\n",
        "        D['order'] = self.order\n",
        "        return D\n",
        "\n",
        "    def connect_xz(self, x):\n",
        "        self.output_z = IndexLayer(self.order)(x)\n",
        "        return self.output_z\n",
        "\n",
        "    def connect_zx(self, z):\n",
        "        self.output_x = IndexLayer(self.reverse)(z)\n",
        "        return self.output_x\n",
        "\n",
        "\n",
        "class SplitChannels(object):\n",
        "    def __init__(self, ndim, nchannels=2, channels=None):\n",
        "        \"\"\" Splits channels forward and merges them backward \"\"\"\n",
        "        self.channels, self.indices_split, self.indices_merge = split_merge_indices(ndim, nchannels=nchannels,\n",
        "                                                                                    channels=channels)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, D):\n",
        "        channels = D['channels']\n",
        "        dim = channels.size\n",
        "        nchannels = channels.max() + 1\n",
        "        return cls(dim, nchannels=nchannels, channels=channels)\n",
        "\n",
        "    def to_dict(self):\n",
        "        D = {}\n",
        "        D['channels'] = self.channels\n",
        "        return D\n",
        "\n",
        "    def connect_xz(self, x):\n",
        "        # split X into different coordinate channels\n",
        "        self.output_z = [IndexLayer(isplit)(x) for isplit in self.indices_split]\n",
        "        return self.output_z\n",
        "\n",
        "    def connect_zx(self, z):\n",
        "        # first concatenate\n",
        "        x_scrambled = keras.layers.Concatenate()(z)\n",
        "        # unscramble x\n",
        "        self.output_x = IndexLayer(self.indices_merge)(x_scrambled) # , name='output_x'\n",
        "        return self.output_x\n",
        "\n",
        "\n",
        "class MergeChannels(SplitChannels):\n",
        "    def connect_xz(self, x):\n",
        "        # first concatenate\n",
        "        z_scrambled = keras.layers.Concatenate()(x)\n",
        "        # unscramble x\n",
        "        self.output_z = IndexLayer(self.indices_merge)(z_scrambled) # , name='output_z'\n",
        "        return self.output_z\n",
        "\n",
        "    def connect_zx(self, z):\n",
        "        # split X into different coordinate channels\n",
        "        self.output_x = [IndexLayer(isplit)(z) for isplit in self.indices_split]\n",
        "        return self.output_x\n",
        "\n",
        "\n",
        "class Scaling(object):\n",
        "    def __init__(self, ndim, scaling_factors=None, trainable=True, name_xz=None, name_zx=None):\n",
        "        \"\"\" Invertible Scaling layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ndim : int\n",
        "            Number of dimensions\n",
        "        scaling_factors : array\n",
        "            Initial scaling factors, must be of shape (1, ndim)\n",
        "        trainable : bool\n",
        "            If True, scaling factors are trainable. If false, they are fixed\n",
        "        name_xz : str\n",
        "            Name for Sxz\n",
        "        name_xz : str\n",
        "            Name for Szx\n",
        "\n",
        "        \"\"\"\n",
        "        # define local classes\n",
        "        class ScalingLayer(keras.engine.Layer):\n",
        "            def __init__(self, log_scaling_factors, **kwargs):\n",
        "                \"\"\" Layer that scales dimensions with trainable factors\n",
        "\n",
        "                Parameters\n",
        "                ----------\n",
        "                scaling_factors : (1xd) array\n",
        "                    scaling factors applied to columns of batch matrix.\n",
        "\n",
        "                \"\"\"\n",
        "                self.log_scaling_factors = log_scaling_factors\n",
        "                super().__init__(**kwargs)\n",
        "\n",
        "            def build(self, input_shape):\n",
        "                # Make weight trainable\n",
        "                if self.trainable:\n",
        "                    self._trainable_weights.append(self.log_scaling_factors)\n",
        "                super().build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "            def compute_output_shape(self, input_shape):\n",
        "                return (input_shape[0], self.log_scaling_factors.shape[1])\n",
        "\n",
        "        class ScalingXZ(ScalingLayer):\n",
        "            def __init__(self, log_scaling_factors, **kwargs):\n",
        "                \"\"\" Layer that scales the batch X in (B,d) by X * S where S=diag(s1,...,sd)\n",
        "                \"\"\"\n",
        "                super().__init__(log_scaling_factors, **kwargs)\n",
        "\n",
        "            def call(self, x):\n",
        "                return x * tf.exp(self.log_scaling_factors)\n",
        "\n",
        "        class ScalingZX(ScalingLayer):\n",
        "            def __init__(self, log_scaling_factors, **kwargs):\n",
        "                \"\"\" Layer that scales the batch X in (B,d) by X * S^(-1) where S=diag(s1,...,sd)\n",
        "                \"\"\"\n",
        "                super().__init__(log_scaling_factors, **kwargs)\n",
        "\n",
        "            def call(self, x):\n",
        "                return x * tf.exp(-self.log_scaling_factors)\n",
        "\n",
        "        # initialize scaling factors\n",
        "        if scaling_factors is None:\n",
        "            self.log_scaling_factors = keras.backend.variable(np.zeros((1, ndim)),\n",
        "                                                              dtype=keras.backend.floatx(),\n",
        "                                                              name='log_scale')\n",
        "        else:\n",
        "            self.log_scaling_factors = keras.backend.variable(np.log(scaling_factors),\n",
        "                                                              dtype=keras.backend.floatx(),\n",
        "                                                              name='log_scale')\n",
        "\n",
        "        self.trainable = trainable\n",
        "        self.Sxz = ScalingXZ(self.log_scaling_factors, trainable=trainable, name=name_xz)\n",
        "        self.Szx = ScalingZX(self.log_scaling_factors, trainable=trainable, name=name_zx)\n",
        "\n",
        "    @property\n",
        "    def scaling_factors(self):\n",
        "        return tf.exp(self.log_scaling_factors)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, D):\n",
        "        scaling_factors = D['scaling_factors']\n",
        "        dim = scaling_factors.shape[1]\n",
        "        trainable = D['trainable']\n",
        "        name_xz = D['name_xz']\n",
        "        name_zx = D['name_zx']\n",
        "        return Scaling(dim, scaling_factors=scaling_factors, trainable=trainable, name_xz=name_xz, name_zx=name_zx)\n",
        "\n",
        "    def to_dict(self):\n",
        "        D = {}\n",
        "        D['scaling_factors'] = keras.backend.eval(self.scaling_factors)\n",
        "        D['trainable'] = self.trainable\n",
        "        D['name_xz'] = self.Sxz.name\n",
        "        D['name_zx'] = self.Szx.name\n",
        "        return D\n",
        "\n",
        "    def connect_xz(self, x):\n",
        "        def lambda_Jxz(x):\n",
        "            J = tf.reduce_sum(input_tensor=self.log_scaling_factors, axis=1)[0]\n",
        "            return J * keras.backend.ones((tf.shape(input=x)[0], 1))\n",
        "        self.log_det_xz = keras.layers.Lambda(lambda_Jxz)(x)\n",
        "        z = self.Sxz(x)\n",
        "        return z\n",
        "\n",
        "    def connect_zx(self, z):\n",
        "        def lambda_Jzx(x):\n",
        "            J = tf.reduce_sum(input_tensor=-self.log_scaling_factors, axis=1)[0]\n",
        "            return J * keras.backend.ones((tf.shape(input=x)[0], 1))\n",
        "        self.log_det_zx = keras.layers.Lambda(lambda_Jzx)(z)\n",
        "        x = self.Szx(z)\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def log_det_Jxz(self):\n",
        "        \"\"\" Log of |det(dz/dx)| for the current batch. Format is batchsize x 1 or a number \"\"\"\n",
        "        return self.log_det_xz\n",
        "\n",
        "    @property\n",
        "    def log_det_Jzx(self):\n",
        "        \"\"\" Log of |det(dx/dz)| for the current batch. Format is batchsize x 1 or a number \"\"\"\n",
        "        return self.log_det_zx\n",
        "\n",
        "\n",
        "class CompositeLayer(object):\n",
        "    def __init__(self, transforms):\n",
        "        \"\"\" Composite layer consisting of multiple keras layers with shared parameters  \"\"\"\n",
        "        self.transforms = transforms\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, d):\n",
        "        from deep_boltzmann.networks.util import deserialize_layers\n",
        "        transforms = deserialize_layers(d['transforms'])\n",
        "        return cls(transforms)\n",
        "\n",
        "    def to_dict(self):\n",
        "        from deep_boltzmann.networks.util import serialize_layers\n",
        "        D = {}\n",
        "        D['transforms'] = serialize_layers(self.transforms)\n",
        "        return D\n",
        "\n",
        "\n",
        "class NICER(CompositeLayer):\n",
        "    def __init__(self, transforms):\n",
        "        \"\"\" Two sequential NICE transformations and their inverse transformations.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        transforms : list\n",
        "            List with [M1, M2] containing the keras layers for nonlinear transformation 1 and 2.\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__(transforms)\n",
        "        self.M1 = transforms[0]\n",
        "        self.M2 = transforms[1]\n",
        "\n",
        "    def connect_xz(self, x):\n",
        "        x1 = x[0]\n",
        "        x2 = x[1]\n",
        "        self.input_x1 = x1\n",
        "        self.input_x2 = x2\n",
        "\n",
        "        # first stage backward\n",
        "        y2 = x2\n",
        "        y1 = keras.layers.Subtract()([x1, connect(x2, self.M2)])\n",
        "        # second stage backward\n",
        "        z1 = y1\n",
        "        z2 = keras.layers.Subtract()([y2, connect(y1, self.M1)])\n",
        "\n",
        "        return [z1, z2] + x[2:]  # append other layers if there are any\n",
        "\n",
        "    def connect_zx(self, z):\n",
        "        z1 = z[0]\n",
        "        z2 = z[1]\n",
        "        self.input_z1 = z1\n",
        "        self.input_z2 = z2\n",
        "\n",
        "        # first stage forward\n",
        "        y1 = z1\n",
        "        y2 = keras.layers.Add()([z2, connect(z1, self.M1)])\n",
        "        # second stage forward\n",
        "        x2 = y2\n",
        "        x1 = keras.layers.Add()([y1, connect(y2, self.M2)])\n",
        "\n",
        "        return [x1, x2] + z[2:]  # append other layers if there are any\n",
        "\n",
        "\n",
        "class RealNVP(CompositeLayer):\n",
        "    def __init__(self, transforms):\n",
        "        \"\"\" Two sequential NVP transformations and their inverse transformatinos.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        transforms : list\n",
        "            List [S1, T1, S2, T2] with keras layers for scaling and translation transforms\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__(transforms)\n",
        "        self.S1 = transforms[0]\n",
        "        self.T1 = transforms[1]\n",
        "        self.S2 = transforms[2]\n",
        "        self.T2 = transforms[3]\n",
        "\n",
        "    def connect_xz(self, x):\n",
        "        def lambda_exp(x):\n",
        "            return keras.backend.exp(x)\n",
        "        def lambda_sum(x):\n",
        "            return keras.backend.sum(x[0], axis=1, keepdims=True) + keras.backend.sum(x[1], axis=1, keepdims=True)\n",
        "\n",
        "        x1 = x[0]\n",
        "        x2 = x[1]\n",
        "        self.input_x1 = x1\n",
        "        self.input_x2 = x2\n",
        "\n",
        "        y1 = x1\n",
        "        self.Sxy_layer = connect(x1, self.S1)\n",
        "        self.Txy_layer = connect(x1, self.T1)\n",
        "        prodx = keras.layers.Multiply()([x2, keras.layers.Lambda(lambda_exp)(self.Sxy_layer)])\n",
        "        y2 = keras.layers.Add()([prodx, self.Txy_layer])\n",
        "\n",
        "        self.output_z2 = y2\n",
        "        self.Syz_layer = connect(y2, self.S2)\n",
        "        self.Tyz_layer = connect(y2, self.T2)\n",
        "        prody = keras.layers.Multiply()([y1, keras.layers.Lambda(lambda_exp)(self.Syz_layer)])\n",
        "        self.output_z1 = keras.layers.Add()([prody, self.Tyz_layer])\n",
        "\n",
        "        # log det(dz/dx)\n",
        "        self.log_det_xz = keras.layers.Lambda(lambda_sum)([self.Sxy_layer, self.Syz_layer])\n",
        "\n",
        "        return [self.output_z1, self.output_z2] + x[2:]  # append other layers if there are any\n",
        "\n",
        "    def connect_zx(self, z):\n",
        "        def lambda_negexp(x):\n",
        "            return keras.backend.exp(-x)\n",
        "        def lambda_negsum(x):\n",
        "            return keras.backend.sum(-x[0], axis=1, keepdims=True) + keras.backend.sum(-x[1], axis=1, keepdims=True)\n",
        "\n",
        "        z1 = z[0]\n",
        "        z2 = z[1]\n",
        "        self.input_z1 = z1\n",
        "        self.input_z2 = z2\n",
        "\n",
        "        y2 = z2\n",
        "        self.Szy_layer = connect(z2, self.S2)\n",
        "        self.Tzy_layer = connect(z2, self.T2)\n",
        "        z1_m_Tz2 = keras.layers.Subtract()([z1, self.Tzy_layer])\n",
        "        y1 = keras.layers.Multiply()([z1_m_Tz2, keras.layers.Lambda(lambda_negexp)(self.Szy_layer)])\n",
        "\n",
        "        self.output_x1 = y1\n",
        "        self.Syx_layer = connect(y1, self.S1)\n",
        "        self.Tyx_layer = connect(y1, self.T1)\n",
        "        y2_m_Ty1 = keras.layers.Subtract()([y2, self.Tyx_layer])\n",
        "        self.output_x2 = keras.layers.Multiply()([y2_m_Ty1, keras.layers.Lambda(lambda_negexp)(self.Syx_layer)])\n",
        "\n",
        "        # log det(dx/dz)\n",
        "        # TODO: check Jacobian\n",
        "        self.log_det_zx = keras.layers.Lambda(lambda_negsum)([self.Szy_layer, self.Syx_layer])\n",
        "\n",
        "        return [self.output_x1, self.output_x2] + z[2:]  # append other layers if there are any\n",
        "\n",
        "    @property\n",
        "    def log_det_Jxz(self):\n",
        "        \"\"\" Log of |det(dz/dx)| for the current batch. Format is batchsize x 1 or a number \"\"\"\n",
        "        return self.log_det_xz\n",
        "\n",
        "    @property\n",
        "    def log_det_Jzx(self):\n",
        "        \"\"\" Log of |det(dx/dz)| for the current batch. Format is batchsize x 1 or a number \"\"\"\n",
        "        return self.log_det_zx"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh0Regw_JjPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# invertible.py\n",
        "\n",
        "class InvNet(object):\n",
        "\n",
        "    def __init__(self, dim, layers, prior='normal'):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        dim : int\n",
        "            Dimension\n",
        "        layers : list\n",
        "            list of invertible layers\n",
        "        prior : str\n",
        "            Type of prior, 'normal', 'lognormal'\n",
        "\n",
        "        \"\"\"\n",
        "        \"\"\" Stack of invertible layers \"\"\"\n",
        "        self.dim = dim\n",
        "        self.layers = layers\n",
        "        self.prior = prior\n",
        "        self.connect_layers()\n",
        "        # compute total Jacobian for x->z transformation\n",
        "        log_det_xzs = []\n",
        "        for l in layers:\n",
        "            if hasattr(l, 'log_det_xz'):\n",
        "                log_det_xzs.append(l.log_det_xz)\n",
        "        if len(log_det_xzs) == 0:\n",
        "            self.TxzJ = None\n",
        "        else:\n",
        "            if len(log_det_xzs) == 1:\n",
        "                self.log_det_xz = log_det_xzs[0]\n",
        "            else:\n",
        "                self.log_det_xz = keras.layers.Add()(log_det_xzs)\n",
        "            self.TxzJ = keras.models.Model(inputs=self.input_x, outputs=[self.output_z, self.log_det_xz])\n",
        "        # compute total Jacobian for z->x transformation\n",
        "        log_det_zxs = []\n",
        "        for l in layers:\n",
        "            if hasattr(l, 'log_det_zx'):\n",
        "                log_det_zxs.append(l.log_det_zx)\n",
        "        if len(log_det_zxs) == 0:\n",
        "            self.TzxJ = None\n",
        "        else:\n",
        "            if len(log_det_zxs) == 1:\n",
        "                self.log_det_zx = log_det_zxs[0]\n",
        "            else:\n",
        "                self.log_det_zx = keras.layers.Add()(log_det_zxs)\n",
        "            self.TzxJ = keras.models.Model(inputs=self.input_z, outputs=[self.output_x, self.log_det_zx])\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, filename, clear_session=True):\n",
        "        \"\"\" Loads parameters into model. Careful: this clears the whole TF session!!\n",
        "        \"\"\"\n",
        "        from deep_boltzmann.util import load_obj\n",
        "        if clear_session:\n",
        "            keras.backend.clear_session()\n",
        "        D = load_obj(filename)\n",
        "        prior = D['prior']\n",
        "        layerdicts = D['layers']\n",
        "        layers = [eval(d['type']).from_dict(d) for d in layerdicts]\n",
        "        return InvNet(D['dim'], layers, prior=prior)\n",
        "\n",
        "    def save(self, filename):\n",
        "        from deep_boltzmann.util import save_obj\n",
        "        D = {}\n",
        "        D['dim'] = self.dim\n",
        "        D['prior'] = self.prior\n",
        "        layerdicts = []\n",
        "        for l in self.layers:\n",
        "            d = l.to_dict()\n",
        "            d['type'] = l.__class__.__name__\n",
        "            layerdicts.append(d)\n",
        "        D['layers'] = layerdicts\n",
        "        save_obj(D, filename)\n",
        "\n",
        "    def connect_xz(self, x):\n",
        "        z = None\n",
        "        for i in range(len(self.layers)):\n",
        "            z = self.layers[i].connect_xz(x)  # connect\n",
        "            #print(self.layers[i])\n",
        "            #print('Inputs\\n', x)\n",
        "            #print()\n",
        "            #print('Outputs\\n', z)\n",
        "            #print('------------')\n",
        "            #print()\n",
        "            x = z  # rename output\n",
        "        return z\n",
        "\n",
        "    def connect_zx(self, z):\n",
        "        x = None\n",
        "        for i in range(len(self.layers)-1, -1, -1):\n",
        "            x = self.layers[i].connect_zx(z)  # connect\n",
        "            #print(self.layers[i])\n",
        "            #print('Inputs\\n', z)\n",
        "            #print()\n",
        "            #print('Outputs\\n', x)\n",
        "            #print('------------')\n",
        "            #print()\n",
        "            z = x  # rename output to next input\n",
        "        return x\n",
        "\n",
        "    def connect_layers(self):\n",
        "        # X -> Z\n",
        "        self.input_x = keras.layers.Input(shape=(self.dim,))\n",
        "        self.output_z = self.connect_xz(self.input_x)\n",
        "\n",
        "        # Z -> X\n",
        "        self.input_z = keras.layers.Input(shape=(self.dim,))\n",
        "        self.output_x = self.connect_zx(self.input_z)\n",
        "\n",
        "        # build networks\n",
        "        self.Txz = keras.models.Model(inputs=self.input_x, outputs=self.output_z)\n",
        "        self.Tzx = keras.models.Model(inputs=self.input_z, outputs=self.output_x)\n",
        "\n",
        "    def predict_log_det_Jxz(self, z):\n",
        "        if self.TzxJ is None:\n",
        "            return np.ones(z.shape[0])\n",
        "        else:\n",
        "            return self.TzxJ.predict(z)[1][:, 0]\n",
        "\n",
        "    @property\n",
        "    def log_det_Jxz(self):\n",
        "        \"\"\" Log of |det(dz/dx)| for the current batch. Format is batchsize x 1 or a number \"\"\"\n",
        "        #return self.log_det_xz.output\n",
        "        log_det_Jxzs = []\n",
        "        for l in self.layers:\n",
        "            if hasattr(l, 'log_det_Jxz'):\n",
        "                log_det_Jxzs.append(l.log_det_Jxz)\n",
        "        if len(log_det_Jxzs) == 0:\n",
        "            return tf.ones((self.output_z.shape[0],))\n",
        "        if len(log_det_Jxzs) == 1:\n",
        "            return log_det_Jxzs[0]\n",
        "        return tf.reduce_sum(input_tensor=log_det_Jxzs, axis=0, keepdims=False)\n",
        "\n",
        "    @property\n",
        "    def log_det_Jzx(self):\n",
        "        \"\"\" Log of |det(dx/dz)| for the current batch. Format is batchsize x 1 or a number \"\"\"\n",
        "        #return self.log_det_zx.output\n",
        "        log_det_Jzxs = []\n",
        "        for l in self.layers:\n",
        "            if hasattr(l, 'log_det_Jzx'):\n",
        "                log_det_Jzxs.append(l.log_det_Jzx)\n",
        "        if len(log_det_Jzxs) == 0:\n",
        "            return tf.ones((self.output_x.shape[0],))\n",
        "        if len(log_det_Jzxs) == 1:\n",
        "            return log_det_Jzxs[0]\n",
        "        return tf.reduce_sum(input_tensor=log_det_Jzxs, axis=0, keepdims=False)\n",
        "\n",
        "    def log_likelihood_z_normal(self, std=1.0):\n",
        "        \"\"\" Returns the log likelihood of z|x assuming a Normal distribution in z\n",
        "        \"\"\"\n",
        "        #return self.log_det_Jxz - self.dim * tf.log(std) - (0.5 / (std**2)) * tf.reduce_sum(self.output_z**2, axis=1)\n",
        "        return self.log_det_Jxz - (0.5 / (std**2)) * tf.reduce_sum(input_tensor=self.output_z**2, axis=1)\n",
        "\n",
        "    def log_likelihood_z_lognormal(self, std=1.0):\n",
        "        \"\"\" Returns the log likelihood of z|x assuming a Normal distribution in z\n",
        "        \"\"\"\n",
        "        #return self.log_det_Jxz - self.dim * tf.log(std) - (0.5 / (std**2)) * tf.reduce_sum(self.output_z**2, axis=1)\n",
        "        from deep_boltzmann.util import logreg\n",
        "        logz = logreg(self.output_z, a=0.001, tf=True)\n",
        "        ll = self.log_det_Jxz \\\n",
        "             - (0.5 / (std**2)) * tf.reduce_sum(input_tensor=logz**2, axis=1) \\\n",
        "             - tf.reduce_sum(input_tensor=logz, axis=1)\n",
        "        return ll\n",
        "\n",
        "    def log_likelihood_z_cauchy(self, scale=1.0):\n",
        "        return -tf.reduce_sum(input_tensor=tf.math.log(1 + (self.output_z / scale)**2), axis=1)\n",
        "\n",
        "    def rc_entropy(self, rc_func, gmeans, gstd, ntemperatures=1):\n",
        "        \"\"\" Computes the entropy along a 1D reaction coordinate\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        rc_func : function\n",
        "            function to compute reaction coordinate\n",
        "        gmeans : array\n",
        "            mean positions of Gauss kernels along reaction coordinate\n",
        "        gstd : float\n",
        "            standard deviation of Gauss kernels along reaction coordinate\n",
        "        \"\"\"\n",
        "        # evaluate rc\n",
        "        rc = rc_func(self.output_x)\n",
        "        rc = tf.expand_dims(rc, axis=1)\n",
        "        # kernelize all values\n",
        "        kmat = tf.exp(-((rc - gmeans)**2) / (2*gstd*gstd))\n",
        "        kmat += 1e-6\n",
        "        kmat /= tf.reduce_sum(input_tensor=kmat, axis=1, keepdims=True)\n",
        "        # distribute counts across temperatures\n",
        "        batchsize_per_temperature = tf.cast(tf.shape(input=kmat)[0] / ntemperatures, tf.int32)\n",
        "        nbins = tf.shape(input=gmeans)[0]\n",
        "        kmatT = tf.transpose(a=tf.reshape(kmat, (batchsize_per_temperature, ntemperatures, nbins)), perm=(1, 0, 2))\n",
        "        histogram = tf.reduce_mean(input_tensor=kmatT, axis=1)\n",
        "        entropies = tf.reduce_sum(input_tensor=tf.math.log(histogram), axis=1)\n",
        "        return tf.reduce_mean(input_tensor=entropies)\n",
        "\n",
        "    def reg_Jzx_uniform(self):\n",
        "        \"\"\" Returns the log likelihood of z|x assuming a Normal distribution in z\n",
        "        \"\"\"\n",
        "        #return self.log_det_Jxz - self.dim * tf.log(std) - (0.5 / (std**2)) * tf.reduce_sum(self.output_z**2, axis=1)\n",
        "        Jmean = tf.reduce_mean(input_tensor=self.log_det_Jzx, axis=0, keepdims=True)\n",
        "        Jdev = tf.reduce_mean(input_tensor=(self.log_det_Jzx - Jmean) ** 2, axis=1, keepdims=False)\n",
        "        return Jdev\n",
        "\n",
        "    def reg_Jxz_uniform(self):\n",
        "        \"\"\" Returns the log likelihood of z|x assuming a Normal distribution in z\n",
        "        \"\"\"\n",
        "        #return self.log_det_Jxz - self.dim * tf.log(std) - (0.5 / (std**2)) * tf.reduce_sum(self.output_z**2, axis=1)\n",
        "        Jmean = tf.reduce_mean(input_tensor=self.log_det_Jxz, axis=0, keepdims=True)\n",
        "        Jdev = tf.reduce_mean(input_tensor=(self.log_det_Jxz - Jmean) ** 2, axis=1, keepdims=False)\n",
        "        return Jdev\n",
        "\n",
        "    def log_likelihood_z_normal_2trajs(self, trajlength, std=1.0):\n",
        "        \"\"\" Returns the log of the sum of two trajectory likelihoods\n",
        "        \"\"\"\n",
        "        #return self.log_det_Jxz - self.dim * tf.log(std) - (0.5 / (std**2)) * tf.reduce_sum(self.output_z**2, axis=1)\n",
        "        J = self.log_det_Jxz\n",
        "        LL1 = tf.reduce_mean(input_tensor=J[:trajlength] - (0.5 / (std**2)) * tf.reduce_sum(input_tensor=self.output_z[:trajlength]**2, axis=1))\n",
        "        LL2 = tf.reduce_mean(input_tensor=J[trajlength:] - (0.5 / (std**2)) * tf.reduce_sum(input_tensor=self.output_z[trajlength:]**2, axis=1))\n",
        "        return tf.reduce_logsumexp(input_tensor=[LL1, LL2])\n",
        "\n",
        "    def train_ML(self, x, xval=None, optimizer=None, lr=0.001, clipnorm=None, epochs=2000, batch_size=1024,\n",
        "                 std=1.0, reg_Jxz=0.0, verbose=1, return_test_energies=False):\n",
        "        if optimizer is None:\n",
        "            if clipnorm is None:\n",
        "                optimizer = keras.optimizers.adam(lr=lr)\n",
        "            else:\n",
        "                optimizer = keras.optimizers.adam(lr=lr, clipnorm=clipnorm)\n",
        "\n",
        "        def loss_ML_normal(y_true, y_pred):\n",
        "            return -self.log_likelihood_z_normal(std=std)\n",
        "        def loss_ML_lognormal(y_true, y_pred):\n",
        "            return -self.log_likelihood_z_lognormal(std=std)\n",
        "        def loss_ML_cauchy(y_true, y_pred):\n",
        "            return -self.log_likelihood_z_cauchy(scale=std)\n",
        "        def loss_ML_normal_reg(y_true, y_pred):\n",
        "            return -self.log_likelihood_z_normal(std=std) + reg_Jxz*self.reg_Jxz_uniform()\n",
        "        def loss_ML_lognormal_reg(y_true, y_pred):\n",
        "            return -self.log_likelihood_z_lognormal(std=std) + reg_Jxz*self.reg_Jxz_uniform()\n",
        "        def loss_ML_cauchy_reg(y_true, y_pred):\n",
        "            return -self.log_likelihood_z_cauchy(scale=std) + reg_Jxz*self.reg_Jxz_uniform()\n",
        "\n",
        "        if self.prior == 'normal':\n",
        "            if reg_Jxz == 0:\n",
        "                self.Txz.compile(optimizer, loss=loss_ML_normal)\n",
        "            else:\n",
        "                self.Txz.compile(optimizer, loss=loss_ML_normal_reg)\n",
        "        elif self.prior == 'lognormal':\n",
        "            if reg_Jxz == 0:\n",
        "                self.Txz.compile(optimizer, loss=loss_ML_lognormal)\n",
        "            else:\n",
        "                self.Txz.compile(optimizer, loss=loss_ML_lognormal_reg)\n",
        "        elif self.prior == 'cauchy':\n",
        "            if reg_Jxz == 0:\n",
        "                self.Txz.compile(optimizer, loss=loss_ML_cauchy)\n",
        "            else:\n",
        "                self.Txz.compile(optimizer, loss=loss_ML_cauchy_reg)\n",
        "        else:\n",
        "            raise NotImplementedError('ML for prior ' + self.prior + ' is not implemented.')\n",
        "\n",
        "        if xval is not None:\n",
        "            validation_data = (xval, np.zeros_like(xval))\n",
        "        else:\n",
        "            validation_data = None\n",
        "\n",
        "        #hist = self.Txz.fit(x=x, y=np.zeros_like(x), validation_data=validation_data,\n",
        "        #                    batch_size=batch_size, epochs=epochs, verbose=verbose, shuffle=True)\n",
        "        # data preprocessing\n",
        "        N = x.shape[0]\n",
        "        I = np.arange(N)\n",
        "        loss_train = []\n",
        "        energies_x_val = []\n",
        "        energies_z_val = []\n",
        "        loss_val = []\n",
        "        y = np.zeros((batch_size, self.dim))\n",
        "        for e in range(epochs):\n",
        "            # sample batch\n",
        "            x_batch = x[np.random.choice(I, size=batch_size, replace=True)]\n",
        "            l = self.Txz.train_on_batch(x=x_batch, y=y)\n",
        "            loss_train.append(l)\n",
        "\n",
        "            # validate\n",
        "            if xval is not None:\n",
        "                xval_batch = xval[np.random.choice(I, size=batch_size, replace=True)]\n",
        "                l = self.Txz.test_on_batch(x=xval_batch, y=y)\n",
        "                loss_val.append(l)\n",
        "                if return_test_energies:\n",
        "                    z = self.sample_z(nsample=batch_size)\n",
        "                    xout = self.transform_zx(z)\n",
        "                    energies_x_val.append(self.energy_model.energy(xout))\n",
        "                    zout = self.transform_xz(xval_batch)\n",
        "                    energies_z_val.append(self.energy_z(zout))\n",
        "\n",
        "            # print\n",
        "            if verbose > 0:\n",
        "                str_ = 'Epoch ' + str(e) + '/' + str(epochs) + ' '\n",
        "                str_ += self.Txz.metrics_names[0] + ' '\n",
        "                str_ += '{:.4f}'.format(loss_train[-1]) + ' '\n",
        "                if xval is not None:\n",
        "                    str_ += '{:.4f}'.format(loss_val[-1]) + ' '\n",
        "#                for i in range(len(self.Txz.metrics_names)):\n",
        "\n",
        "                    #str_ += self.Txz.metrics_names[i] + ' '\n",
        "                    #str_ += '{:.4f}'.format(loss_train[-1][i]) + ' '\n",
        "                    #if xval is not None:\n",
        "                    #    str_ += '{:.4f}'.format(loss_val[-1][i]) + ' '\n",
        "                print(str_)\n",
        "                sys.stdout.flush()\n",
        "\n",
        "        if return_test_energies:\n",
        "            return loss_train, loss_val, energies_x_val, energies_z_val\n",
        "        else:\n",
        "            return loss_train, loss_val\n",
        "\n",
        "    def transform_xz(self, x):\n",
        "        return self.Txz.predict(ensure_traj(x))\n",
        "\n",
        "    def transform_xzJ(self, x):\n",
        "        x = ensure_traj(x)\n",
        "        if self.TxzJ is None:\n",
        "            return self.Txz.predict(x), np.zeros(x.shape[0])\n",
        "        else:\n",
        "            z, J = self.TxzJ.predict(x)\n",
        "            return z, J[:, 0]\n",
        "\n",
        "    def transform_zx(self, z):\n",
        "        return self.Tzx.predict(ensure_traj(z))\n",
        "\n",
        "    def transform_zxJ(self, z):\n",
        "        z = ensure_traj(z)\n",
        "        if self.TxzJ is None:\n",
        "            return self.Tzx.predict(z), np.zeros(z.shape[0])\n",
        "        else:\n",
        "            x, J = self.TzxJ.predict(z)\n",
        "            return x, J[:, 0]\n",
        "\n",
        "    def std_z(self, x):\n",
        "        \"\"\" Computes average standard deviation from the origin in z for given x \"\"\"\n",
        "        z = self.Txz.predict(x)\n",
        "        sigma = np.mean(z**2, axis=0)\n",
        "        z_std_ = np.sqrt(np.mean(sigma))\n",
        "        return z_std_\n",
        "\n",
        "    def energy_z(self, z, temperature=1.0):\n",
        "        if self.prior == 'normal':\n",
        "            E = self.dim * np.log(np.sqrt(temperature)) + np.sum(z**2 / (2*temperature), axis=1)\n",
        "        elif self.prior == 'lognormal':\n",
        "            sample_z_normal = np.log(z)\n",
        "            E = np.sum(sample_z_normal**2 / (2*temperature), axis=1) + np.sum(sample_z_normal, axis=1)\n",
        "        elif self.prior == 'cauchy':\n",
        "            E = np.sum(np.log(1 + (z/temperature)**2), axis=1)\n",
        "        return E\n",
        "\n",
        "    def sample_z(self, temperature=1.0, nsample=100000, return_energy=False):\n",
        "        \"\"\" Samples from prior distribution in x and produces generated x configurations\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        temperature : float\n",
        "            Relative temperature. Equal to the variance of the isotropic Gaussian sampled in z-space.\n",
        "        nsample : int\n",
        "            Number of samples\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        sample_z : array\n",
        "            Samples in z-space\n",
        "        energy_z : array\n",
        "            Energies of z samples (optional)\n",
        "\n",
        "        \"\"\"\n",
        "        sample_z = None\n",
        "        energy_z = None\n",
        "        if self.prior == 'normal':\n",
        "            sample_z = np.sqrt(temperature) * np.random.randn(nsample, self.dim)\n",
        "        elif self.prior == 'lognormal':\n",
        "            sample_z_normal = np.sqrt(temperature) * np.random.randn(nsample, self.dim)\n",
        "            sample_z = np.exp(sample_z_normal)\n",
        "        elif self.prior == 'cauchy':\n",
        "            from scipy.stats import cauchy\n",
        "            sample_z = cauchy(loc=0, scale=temperature).rvs(size=(nsample, self.dim))\n",
        "        else:\n",
        "            raise NotImplementedError('Sampling for prior ' + self.prior + ' is not implemented.')\n",
        "\n",
        "        if return_energy:\n",
        "            E = self.energy_z(sample_z)\n",
        "            return sample_z, E\n",
        "        else:\n",
        "            return sample_z\n",
        "\n",
        "class EnergyInvNet(InvNet):\n",
        "\n",
        "    def __init__(self, energy_model, layers, prior='normal'):\n",
        "        \"\"\" Invertible net where we have an energy function that defines p(x) \"\"\"\n",
        "        self.energy_model = energy_model\n",
        "        super().__init__(energy_model.dim, layers, prior=prior)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, filename, energy_model, clear_session=True):\n",
        "        \"\"\" Loads parameters into model. Careful: this clears the whole TF session!!\n",
        "        \"\"\"\n",
        "        from deep_boltzmann.util import load_obj\n",
        "        if clear_session:\n",
        "                keras.backend.clear_session()\n",
        "        D = load_obj(filename)\n",
        "        prior = D['prior']\n",
        "        layerdicts = D['layers']\n",
        "        layers = [eval(d['type']).from_dict(d) for d in layerdicts]\n",
        "        return EnergyInvNet(energy_model, layers, prior=prior)\n",
        "\n",
        "    # TODO: This is only implemented for the normal prior.\n",
        "    def log_w(self, high_energy, max_energy, temperature_factors=1.0):\n",
        "        \"\"\" Computes the variance of the log reweighting factors\n",
        "        \"\"\"\n",
        "        from deep_boltzmann.util import linlogcut\n",
        "        z = self.input_z\n",
        "        x = self.output_x\n",
        "        # compute z energy\n",
        "        Ez = self.dim * tf.math.log(tf.sqrt(temperature_factors)) + tf.reduce_sum(input_tensor=z**2, axis=1) / (2.0 * temperature_factors)\n",
        "        # compute x energy and regularize\n",
        "        Ex = self.energy_model.energy_tf(x) / temperature_factors\n",
        "        Exreg = linlogcut(Ex, high_energy, max_energy, tf=True)\n",
        "        # log weight\n",
        "        log_w = -Exreg + Ez + self.log_det_Jzx[:, 0]\n",
        "        return log_w\n",
        "\n",
        "    def sample(self, temperature=1.0, nsample=100000):\n",
        "        \"\"\" Samples from prior distribution in x and produces generated x configurations\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        temperature : float\n",
        "            Relative temperature. Equal to the variance of the isotropic Gaussian sampled in z-space.\n",
        "        nsample : int\n",
        "            Number of samples\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        sample_z : array\n",
        "            Samples in z-space\n",
        "        sample_x : array\n",
        "            Samples in x-space\n",
        "        energy_z : array\n",
        "            Energies of z samples\n",
        "        energy_x : array\n",
        "            Energies of x samples\n",
        "        log_w : array\n",
        "            Log weight of samples\n",
        "\n",
        "        \"\"\"\n",
        "        sample_z, energy_z = self.sample_z(temperature=temperature, nsample=nsample, return_energy=True)\n",
        "        sample_x, Jzx = self.transform_zxJ(sample_z)\n",
        "        energy_x = self.energy_model.energy(sample_x) / temperature\n",
        "        logw = -energy_x + energy_z + Jzx\n",
        "\n",
        "        return sample_z, sample_x, energy_z, energy_x, logw\n",
        "\n",
        "    def log_KL_x(self, high_energy, max_energy, temperature_factors=1.0, explore=1.0):\n",
        "        \"\"\" Computes the KL divergence with respect to z|x and the Boltzmann distribution\n",
        "        \"\"\"\n",
        "        x = self.output_x\n",
        "        # compute energy\n",
        "        E = self.energy_model.energy_tf(x) / temperature_factors\n",
        "        # regularize using log\n",
        "        Ereg = linlogcut(E, high_energy, max_energy, tf=True)\n",
        "        #Ereg = _linlogcut_tf_constantclip(E, high_energy, max_energy)\n",
        "        # gradient_clip(bg1.energy_model.energy_tf, 1e16, 1e20)\n",
        "        #return self.log_det_Jzx + Ereg\n",
        "        return -explore * self.log_det_Jzx[:, 0] + Ereg\n",
        "\n",
        "    def log_GaussianPriorMCMC_efficiency(self, high_energy, max_energy, metric=None, symmetric=False):\n",
        "        \"\"\" Computes the efficiency of GaussianPriorMCMC from a parallel x1->z1, z2->x2 network.\n",
        "\n",
        "        If metric is given, computes the efficiency as distance + log p_acc, where distance\n",
        "        is computed by |x1-x2|**2\n",
        "\n",
        "        \"\"\"\n",
        "        from deep_boltzmann.util import linlogcut\n",
        "        # define variables\n",
        "        x1 = self.input_x\n",
        "        x2 = self.output_x\n",
        "        z1 = self.output_z\n",
        "        z2 = self.input_z\n",
        "        # prior entropies\n",
        "        H1 = 0.5 * tf.reduce_sum(input_tensor=z1**2, axis=1)\n",
        "        H2 = 0.5 * tf.reduce_sum(input_tensor=z2**2, axis=1)\n",
        "        # compute and regularize energies\n",
        "        E1 = self.energy_model.energy_tf(x1)\n",
        "        E1reg = linlogcut(E1, high_energy, max_energy, tf=True)\n",
        "        E2 = self.energy_model.energy_tf(x2)\n",
        "        E2reg = linlogcut(E2, high_energy, max_energy, tf=True)\n",
        "        # free energy of samples\n",
        "        F1 = E1reg - H1 + self.log_det_xz[:, 0]\n",
        "        F2 = E2reg - H2 - self.log_det_zx[:, 0]\n",
        "        # acceptance probability\n",
        "        if symmetric:\n",
        "            arg1 = linlogcut(F2 - F1, 10, 1000, tf=True)\n",
        "            arg2 = linlogcut(F1 - F2, 10, 1000, tf=True)\n",
        "            log_pacc = -tf.math.log(1 + tf.exp(arg1)) - tf.math.log(1 + tf.exp(arg2))\n",
        "        else:\n",
        "            arg = linlogcut(F2 - F1, 10, 1000, tf=True)\n",
        "            log_pacc = -tf.math.log(1 + tf.exp(arg))\n",
        "        # mean square distance\n",
        "        if metric is None:\n",
        "            return log_pacc\n",
        "        else:\n",
        "            d = (metric(x1) - metric(x2)) ** 2\n",
        "            return d + log_pacc\n",
        "\n",
        "    def log_GaussianPriorMCMC_efficiency_unsupervised(self, high_energy, max_energy, metric=None):\n",
        "        \"\"\" Computes the efficiency of GaussianPriorMCMC\n",
        "        \"\"\"\n",
        "        from deep_boltzmann.util import linlogcut\n",
        "        # prior entropy\n",
        "        z = self.input_z\n",
        "        H = 0.5 * tf.reduce_sum(input_tensor=z**2, axis=1)\n",
        "        # compute and regularize energy\n",
        "        x = self.output_x\n",
        "        E = self.energy_model.energy_tf(x)\n",
        "        J = self.log_det_Jzx[:, 0]\n",
        "        Ereg = linlogcut(E, high_energy, max_energy, tf=True)\n",
        "        # free energy of samples\n",
        "        F = Ereg - H - J\n",
        "        # acceptance probability\n",
        "        arg = linlogcut(F[1:] - F[:-1], 10, 1000, tf=True)\n",
        "        log_pacc = -tf.math.log(1 + tf.exp(arg))\n",
        "        # mean square distance\n",
        "        # log_dist2 = tf.log(tf.reduce_mean((x[1:] - x[:-1])**2, axis=1))\n",
        "        # complement with 0's\n",
        "        log_pacc_0_ = tf.concat([np.array([0], dtype=np.float32), log_pacc], 0)\n",
        "        log_pacc__0 = tf.concat([log_pacc, np.array([0], dtype=np.float32)], 0)\n",
        "        if metric is None:\n",
        "            return log_pacc_0_ + log_pacc__0\n",
        "        else:\n",
        "            d = (metric(x)[1:] - metric(x)[:1]) ** 2\n",
        "            d_0_ = tf.concat([np.array([0], dtype=np.float32), d], 0)\n",
        "            d__0 = tf.concat([d, np.array([0], dtype=np.float32)], 0)\n",
        "            return d_0_ + d__0 + log_pacc_0_ + log_pacc__0\n",
        "\n",
        "    def train_KL(self, optimizer=None, lr=0.001, epochs=2000, batch_size=1024, verbose=1, clipnorm=None,\n",
        "                 high_energy=100, max_energy=1e10, temperature=1.0, explore=1.0):\n",
        "        if optimizer is None:\n",
        "            if clipnorm is None:\n",
        "                optimizer = keras.optimizers.adam(lr=lr)\n",
        "            else:\n",
        "                optimizer = keras.optimizers.adam(lr=lr, clipnorm=clipnorm)\n",
        "\n",
        "        import numbers\n",
        "        if isinstance(temperature, numbers.Number):\n",
        "            temperature = np.array([temperature])\n",
        "        else:\n",
        "            temperature = np.array(temperature)\n",
        "        tfac = np.tile(temperature, int(batch_size / temperature.size) + 1)[:batch_size]\n",
        "\n",
        "        def loss_KL(y_true, y_pred):\n",
        "            return self.log_KL_x(high_energy, max_energy, temperature_factors=tfac, explore=explore)\n",
        "\n",
        "        self.Tzx.compile(optimizer, loss=loss_KL)\n",
        "\n",
        "        dummy_output = np.zeros((batch_size, self.dim))\n",
        "        train_loss = []\n",
        "        samples = []\n",
        "        for e in range(epochs):\n",
        "            # train in batches\n",
        "            #w = np.sqrt(tfac)[:, None] * np.random.randn(batch_size, self.dim)\n",
        "            w = self.sample_z(temperature=tfac[:, None], nsample=batch_size, return_energy=False)\n",
        "            x = self.transform_zx(w)\n",
        "            samples.append(x)\n",
        "            # w = np.random.randn(batch_size, self.dim)\n",
        "            train_loss_batch = self.Tzx.train_on_batch(x=w, y=dummy_output)\n",
        "            train_loss.append(train_loss_batch)\n",
        "            if verbose == 1:\n",
        "                print('Epoch', e, ' loss', np.mean(train_loss_batch))\n",
        "                sys.stdout.flush()\n",
        "        train_loss = np.array(train_loss)\n",
        "        samples = np.array(samples)\n",
        "\n",
        "        return train_loss, samples\n",
        "\n",
        "    def train_flexible(self, x, xval=None, optimizer=None, lr=0.001, epochs=2000, batch_size=1024, verbose=1,\n",
        "                       clipnorm=None, high_energy=100, max_energy=1e10, std=1.0, reg_Jxz=0.0,\n",
        "                       weight_ML=1.0,\n",
        "                       weight_KL=1.0, temperature=1.0, explore=1.0,\n",
        "                       weight_MC=0.0, metric=None, symmetric_MC=False, supervised_MC=True,\n",
        "                       weight_W2=0.0,\n",
        "                       weight_RCEnt=0.0, rc_func=None, rc_min=0.0, rc_max=1.0,\n",
        "                       return_test_energies=False):\n",
        "        import numbers\n",
        "        if isinstance(temperature, numbers.Number):\n",
        "            temperature = np.array([temperature])\n",
        "        else:\n",
        "            temperature = np.array(temperature)\n",
        "        temperature = temperature.astype(np.float32)\n",
        "        # redefine batch size to be a multiple of temperatures\n",
        "        batch_size_per_temp = int(batch_size / temperature.size)\n",
        "        batch_size = int(temperature.size * batch_size_per_temp)\n",
        "        tidx = np.tile(np.arange(temperature.size), batch_size_per_temp)\n",
        "        tfac = temperature[tidx]\n",
        "\n",
        "        # Assemble Loss function\n",
        "        def loss_ML(y_true, y_pred):\n",
        "            return -self.log_likelihood_z_normal(std=std)\n",
        "        def loss_ML_reg(y_true, y_pred):\n",
        "            return -self.log_likelihood_z_normal(std=std) + reg_Jxz*self.reg_Jxz_uniform()\n",
        "        def loss_KL(y_true, y_pred):\n",
        "            return self.log_KL_x(high_energy, max_energy, temperature_factors=tfac, explore=explore)\n",
        "        def loss_MCEff_supervised(y_true, y_pred):\n",
        "            return -self.log_GaussianPriorMCMC_efficiency(high_energy, max_energy, metric=metric, symmetric=symmetric_MC)\n",
        "        def loss_MCEff_unsupervised(y_true, y_pred):\n",
        "            return -self.log_GaussianPriorMCMC_efficiency_unsupervised(high_energy, max_energy, metric=metric)\n",
        "        def loss_MCEff_combined(y_true, y_pred):\n",
        "            return -self.log_GaussianPriorMCMC_efficiency(high_energy, max_energy, metric=metric, symmetric=symmetric_MC) \\\n",
        "                   -0.5 * self.log_GaussianPriorMCMC_efficiency_unsupervised(high_energy, max_energy, metric=metric)\n",
        "        def loss_W2_var(y_true, y_pred):\n",
        "            # compute all reweighting factors\n",
        "            lw = self.log_w(high_energy, max_energy, temperature_factors=tfac)\n",
        "            # reshape to a column per temperature\n",
        "            lwT = tf.reshape(lw, (batch_size_per_temp, temperature.size))\n",
        "            lwT_mean = tf.reduce_mean(input_tensor=lwT, axis=0, keepdims=True)\n",
        "            return tf.reduce_mean(input_tensor=(lwT - lwT_mean) ** 2)\n",
        "        def loss_W2_dev(y_true, y_pred):\n",
        "            # compute all reweighting factors\n",
        "            lw = self.log_w(high_energy, max_energy, temperature_factors=tfac)\n",
        "            # reshape to a column per temperature\n",
        "            lwT = tf.reshape(lw, (batch_size_per_temp, temperature.size))\n",
        "            lwT_mean = tf.reduce_mean(input_tensor=lwT, axis=0, keepdims=True)\n",
        "            return tf.reduce_mean(input_tensor=tf.abs(lwT - lwT_mean))\n",
        "        gmeans = None\n",
        "        gstd = 0.0\n",
        "        if weight_RCEnt > 0.0:\n",
        "            gmeans = np.linspace(rc_min, rc_max, 11)\n",
        "            gstd = (rc_max - rc_min) / 11.0\n",
        "        def loss_RCEnt(y_true, y_pred):\n",
        "            return -self.rc_entropy(rc_func, gmeans, gstd, temperature.size)\n",
        "        inputs = []\n",
        "        outputs = []\n",
        "        losses = []\n",
        "        loss_weights = []\n",
        "        if weight_ML > 0:\n",
        "            inputs.append(self.input_x)\n",
        "            outputs.append(self.output_z)\n",
        "            if reg_Jxz == 0:\n",
        "                losses.append(loss_ML)\n",
        "            else:\n",
        "                losses.append(loss_ML_reg)\n",
        "            loss_weights.append(weight_ML)\n",
        "        if weight_KL > 0:\n",
        "            inputs.append(self.input_z)\n",
        "            outputs.append(self.output_x)\n",
        "            losses.append(loss_KL)\n",
        "            loss_weights.append(weight_KL)\n",
        "        if weight_MC > 0:\n",
        "            if self.input_z not in inputs:\n",
        "                inputs.append(self.input_z)\n",
        "            #if self.output_x not in outputs:\n",
        "            outputs.append(self.output_x)\n",
        "            if supervised_MC == 'both':\n",
        "                losses.append(loss_MCEff_combined)\n",
        "            elif supervised_MC is True:\n",
        "                losses.append(loss_MCEff_supervised)\n",
        "            else:\n",
        "                losses.append(loss_MCEff_unsupervised)\n",
        "            loss_weights.append(weight_MC)\n",
        "        if weight_W2 > 0:\n",
        "            if self.input_z not in inputs:\n",
        "                inputs.append(self.input_z)\n",
        "            #if self.output_x not in outputs:\n",
        "            outputs.append(self.output_x)\n",
        "            losses.append(loss_W2_dev)\n",
        "            loss_weights.append(weight_W2)\n",
        "        if weight_RCEnt > 0:\n",
        "            if self.input_z not in inputs:\n",
        "                inputs.append(self.input_z)\n",
        "            #if self.output_x not in outputs:\n",
        "            outputs.append(self.output_x)\n",
        "            losses.append(loss_RCEnt)\n",
        "            loss_weights.append(weight_RCEnt)\n",
        "\n",
        "        # data preprocessing\n",
        "        N = x.shape[0]\n",
        "        I = np.arange(N)\n",
        "        if xval is not None:\n",
        "            Nval = xval.shape[0]\n",
        "            Ival = np.arange(N)\n",
        "        else:\n",
        "            Nval = 0\n",
        "            Ival = None\n",
        "\n",
        "        # build estimator\n",
        "        if optimizer is None:\n",
        "            if clipnorm is None:\n",
        "                optimizer = keras.optimizers.adam(lr=lr)\n",
        "            else:\n",
        "                optimizer = keras.optimizers.adam(lr=lr, clipnorm=clipnorm)\n",
        "\n",
        "        # assemble model\n",
        "        dual_model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "        dual_model.compile(optimizer=optimizer, loss=losses, loss_weights=loss_weights)\n",
        "\n",
        "        # training loop\n",
        "        dummy_output = np.zeros((batch_size, self.dim))\n",
        "        y = [dummy_output for o in outputs]\n",
        "        loss_train = []\n",
        "        energies_x_val = []\n",
        "        energies_z_val = []\n",
        "        loss_val = []\n",
        "        for e in range(epochs):\n",
        "            # sample batch\n",
        "            x_batch = x[np.random.choice(I, size=batch_size, replace=True)]\n",
        "            w_batch = np.sqrt(tfac)[:, None] * np.random.randn(batch_size, self.dim)\n",
        "            l = dual_model.train_on_batch(x=[x_batch, w_batch], y=y)\n",
        "            loss_train.append(l)\n",
        "\n",
        "            # validate\n",
        "            if xval is not None:\n",
        "                xval_batch = xval[np.random.choice(I, size=batch_size, replace=True)]\n",
        "                wval_batch = np.sqrt(tfac)[:, None] * np.random.randn(batch_size, self.dim)\n",
        "                l = dual_model.test_on_batch(x=[xval_batch, wval_batch], y=y)\n",
        "                loss_val.append(l)\n",
        "                if return_test_energies:\n",
        "                    xout = self.transform_zx(wval_batch)\n",
        "                    energies_x_val.append(self.energy_model.energy(xout))\n",
        "                    zout = self.transform_xz(xval_batch)\n",
        "                    energies_z_val.append(self.energy_z(zout))\n",
        "\n",
        "            # print\n",
        "            if verbose > 0:\n",
        "                str_ = 'Epoch ' + str(e) + '/' + str(epochs) + ' '\n",
        "                for i in range(len(dual_model.metrics_names)):\n",
        "                    str_ += dual_model.metrics_names[i] + ' '\n",
        "                    str_ += '{:.4f}'.format(loss_train[-1][i]) + ' '\n",
        "                    if xval is not None:\n",
        "                        str_ += '{:.4f}'.format(loss_val[-1][i]) + ' '\n",
        "                print(str_)\n",
        "                sys.stdout.flush()\n",
        "\n",
        "        if return_test_energies:\n",
        "            return dual_model.metrics_names, np.array(loss_train), np.array(loss_val), energies_x_val, energies_z_val\n",
        "        else:\n",
        "            return dual_model.metrics_names, np.array(loss_train), np.array(loss_val)\n",
        "\n",
        "def invnet(dim, layer_types, energy_model=None, channels=None,\n",
        "           nl_layers=2, nl_hidden=100, nl_layers_scale=None, nl_hidden_scale=None,\n",
        "           nl_activation='relu', nl_activation_scale='tanh', scale=None, prior='normal',\n",
        "           permute_atomwise=False, permute_order=None,\n",
        "           whiten=None, whiten_keepdims=None,\n",
        "           ic=None, ic_cart=None, ic_norm=None, ic_cart_norm=None, torsion_cut=None, ic_jacobian_regularizer=1e-10,\n",
        "           rg_splitfrac=0.5,\n",
        "           **layer_args):\n",
        "    \"\"\"\n",
        "    layer_types : str\n",
        "        String describing the sequence of layers. Usage:\n",
        "            N NICER layer\n",
        "            n NICER layer, share parameters with last layer\n",
        "            R RealNVP layer\n",
        "            r RealNVP layer, share parameters with last layer\n",
        "            S Scaling layer\n",
        "            W Whiten layer\n",
        "            P Permute layer\n",
        "            Z Split dimensions off to latent space, leads to a merge and 3-way split.\n",
        "        Splitting and merging layers will be added automatically\n",
        "    energy_model : Energy model class\n",
        "        Class with energy() and dim\n",
        "    channels : array or None\n",
        "        Assignment of dimensions to channels (0/1 array of length ndim)\n",
        "    nl_layers : int\n",
        "        Number of hidden layers in the nonlinear transformations\n",
        "    nl_hidden : int\n",
        "        Number of hidden units in each nonlinear layer\n",
        "    nl_activation : str\n",
        "        Hidden-neuron activation functions used in the nonlinear layers\n",
        "    nl_activation_scale : str\n",
        "        Hidden-neuron activation functions used in scaling networks. If None, nl_activation will be used.\n",
        "    scale : None or float\n",
        "        If a scaling layer is used, fix the scale to this number. If None, scaling layers are trainable\n",
        "    prior : str\n",
        "        Form of the prior distribution\n",
        "    whiten : None or array\n",
        "        If not None, compute a whitening transformation with respect togiven coordinates\n",
        "    whiten_keepdims : None or int\n",
        "        Number of largest-variance dimensions to keep after whitening.\n",
        "    ic : None or array\n",
        "        If not None, compute internal coordinates using this Z index matrix. Do not mix with whitening.\n",
        "    ic_cart : None or array\n",
        "        If not None, use cartesian coordinates and whitening for these atoms.\n",
        "    ic_norm : None or array\n",
        "        If not None, these x coordinates will be used to compute the IC mean and std. These will be used\n",
        "        for normalization\n",
        "    torsion_cut : None or aray\n",
        "        If given defines where the torsions are cut\n",
        "    rg_splitfrac : float\n",
        "        Splitting fraction for Z layers\n",
        "\n",
        "    \"\"\"\n",
        "    # fix channels\n",
        "    channels, indices_split, indices_merge = split_merge_indices(dim, nchannels=2, channels=channels)\n",
        "\n",
        "    # augment layer types with split and merge layers\n",
        "    split = False\n",
        "    tmp = ''\n",
        "    if whiten is not None:\n",
        "        tmp += 'W'\n",
        "    if ic is not None:\n",
        "        tmp += 'I'\n",
        "    for ltype in layer_types:\n",
        "        if (ltype == 'S' or ltype == 'P') and split:\n",
        "            tmp += '>'\n",
        "            split = False\n",
        "        if (ltype == 'N' or ltype == 'R') and not split:\n",
        "            tmp += '<'\n",
        "            split = True\n",
        "        tmp += ltype\n",
        "    if split:\n",
        "        tmp += '>'\n",
        "    layer_types = tmp\n",
        "    print(layer_types)\n",
        "\n",
        "    # prepare layers\n",
        "    layers = []\n",
        "\n",
        "    if nl_activation_scale is None:\n",
        "        nl_activation_scale = nl_activation\n",
        "    if nl_layers_scale is None:\n",
        "        nl_layers_scale = nl_layers\n",
        "    if nl_hidden_scale is None:\n",
        "        nl_hidden_scale= nl_hidden\n",
        "\n",
        "    # number of dimensions left in the signal. The remaining dimensions are going to latent space directly\n",
        "    dim_L = dim\n",
        "    dim_R = 0\n",
        "    dim_Z = 0\n",
        "\n",
        "    # translate and scale layers\n",
        "    T1 = None\n",
        "    T2 = None\n",
        "    S1 = None\n",
        "    S2 = None\n",
        "\n",
        "    for ltype in layer_types:\n",
        "        print(ltype, dim_L, dim_R, dim_Z)\n",
        "        if ltype == '<':\n",
        "            if dim_R > 0:\n",
        "                raise RuntimeError('Already split. Cannot invoke split layer.')\n",
        "            channels_cur = np.concatenate([channels[:dim_L], np.tile([2], dim_Z)])\n",
        "            dim_L = np.count_nonzero(channels_cur==0)\n",
        "            dim_R = np.count_nonzero(channels_cur==1)\n",
        "            layers.append(SplitChannels(dim, channels=channels_cur))\n",
        "        elif ltype == '>':\n",
        "            if dim_R == 0:\n",
        "                raise RuntimeError('Not split. Cannot invoke merge layer.')\n",
        "            channels_cur = np.concatenate([channels[:(dim_L+dim_R)], np.tile([2], dim_Z)])\n",
        "            dim_L += dim_R\n",
        "            dim_R = 0\n",
        "            layers.append(MergeChannels(dim, channels=channels_cur))\n",
        "        elif ltype == 'P':\n",
        "            if permute_atomwise:\n",
        "                order_atomwise = np.arange(dim).reshape((dim//3, 3))\n",
        "                permut_ = np.random.choice(dim//3, dim//3, replace=False)\n",
        "                layers.append(Permute(dim, order=order_atomwise[permut_, :].flatten() ))\n",
        "            else:\n",
        "                if dim_Z > 0 and permute_order is None:\n",
        "                    order = np.concatenate([np.random.choice(dim-dim_Z, dim-dim_Z, replace=False),\n",
        "                                            np.arange(dim-dim_Z, dim)])\n",
        "                layers.append(Permute(dim, order=permute_order))\n",
        "        elif ltype == 'N':\n",
        "            if dim_R == 0:\n",
        "                raise RuntimeError('Not split. Cannot invoke NICE layer.')\n",
        "            T1 = nonlinear_transform(dim_R, nlayers=nl_layers, nhidden=nl_hidden,\n",
        "                                     activation=nl_activation, **layer_args)\n",
        "            T2 = nonlinear_transform(dim_L, nlayers=nl_layers, nhidden=nl_hidden,\n",
        "                                     activation=nl_activation, **layer_args)\n",
        "            layers.append(NICER([T1, T2]))\n",
        "        elif ltype == 'n':\n",
        "            if dim_R == 0:\n",
        "                raise RuntimeError('Not split. Cannot invoke NICE layer.')\n",
        "            layers.append(NICER([T1, T2]))\n",
        "        elif ltype == 'R':\n",
        "            if dim_R == 0:\n",
        "                raise RuntimeError('Not split. Cannot invoke RealNVP layer.')\n",
        "            S1 = nonlinear_transform(dim_R, nlayers=nl_layers_scale, nhidden=nl_hidden_scale,\n",
        "                                     activation=nl_activation_scale, init_outputs=0, **layer_args)\n",
        "            T1 = nonlinear_transform(dim_R, nlayers=nl_layers, nhidden=nl_hidden,\n",
        "                                     activation=nl_activation, **layer_args)\n",
        "            S2 = nonlinear_transform(dim_L, nlayers=nl_layers_scale, nhidden=nl_hidden_scale,\n",
        "                                     activation=nl_activation_scale, init_outputs=0, **layer_args)\n",
        "            T2 = nonlinear_transform(dim_L, nlayers=nl_layers, nhidden=nl_hidden,\n",
        "                                     activation=nl_activation, **layer_args)\n",
        "            layers.append(RealNVP([S1, T1, S2, T2]))\n",
        "        elif ltype == 'r':\n",
        "            if dim_R == 0:\n",
        "                raise RuntimeError('Not split. Cannot invoke RealNVP layer.')\n",
        "            layers.append(RealNVP([S1, T1, S2, T2]))\n",
        "        elif ltype == 'S':\n",
        "            if dim_R > 0:\n",
        "                raise RuntimeError('Not merged. Cannot invoke constant scaling layer.')\n",
        "            # scaling layer\n",
        "            if scale is None:\n",
        "                scaling_factors = None\n",
        "            else:\n",
        "                scaling_factors = scale * np.ones((1, dim))\n",
        "            layers.append(Scaling(dim, scaling_factors=scaling_factors, trainable=(scale is None)))\n",
        "        elif ltype == 'I':\n",
        "            if dim_R > 0:\n",
        "                raise RuntimeError('Already split. Cannot invoke IC layer.')\n",
        "            dim_L = dim_L - 6\n",
        "            dim_R = 0\n",
        "            dim_Z = 6\n",
        "            if ic_cart is None:\n",
        "                layer = InternalCoordinatesTransformation(ic, Xnorm=ic_norm, torsion_cut=torsion_cut)\n",
        "            else:\n",
        "                if ic_cart_norm is None:\n",
        "                    ic_cart_norm = ic_norm\n",
        "                layer = MixedCoordinatesTransformation(ic_cart, ic, X0=ic_cart_norm, X0ic=ic_norm, torsion_cut=torsion_cut, jacobian_regularizer=ic_jacobian_regularizer)\n",
        "            layers.append(layer)\n",
        "        elif ltype == 'W':\n",
        "            if dim_R > 0:\n",
        "                raise RuntimeError('Not merged. Cannot invoke whitening layer.')\n",
        "            W = FixedWhiten(whiten, keepdims=whiten_keepdims)\n",
        "            dim_L = W.keepdims\n",
        "            dim_Z = dim-W.keepdims\n",
        "            layers.append(W)\n",
        "        elif ltype == 'Z':\n",
        "            if dim_R == 0:\n",
        "                raise RuntimeError('Not split. Cannot invoke Z layer.')\n",
        "            if dim_L + dim_R <= 1:  # nothing left to split, so we ignore this layer and move on\n",
        "                break\n",
        "            # merge the current pattern\n",
        "            channels_cur = np.concatenate([channels[:(dim_L+dim_R)], np.tile([2], dim_Z)])\n",
        "            dim_L += dim_R\n",
        "            dim_R = 0\n",
        "            layers.append(MergeChannels(dim, channels=channels_cur))\n",
        "            # 3-way split\n",
        "            split_to_z = int((dim_L + dim_R) * rg_splitfrac)\n",
        "            split_to_z = max(1, split_to_z)  # split at least 1 dimension\n",
        "            dim_Z += split_to_z\n",
        "            dim_L -= split_to_z\n",
        "            channels_cur = np.concatenate([channels[:dim_L], np.tile([2], dim_Z)])\n",
        "            dim_L = np.count_nonzero(channels_cur==0)\n",
        "            dim_R = np.count_nonzero(channels_cur==1)\n",
        "            layers.append(SplitChannels(dim, channels=channels_cur))\n",
        "\n",
        "    if energy_model is None:\n",
        "        return InvNet(dim, layers, prior=prior)\n",
        "    else:\n",
        "        return EnergyInvNet(energy_model, layers, prior=prior)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJIhgobO-sc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "2206e42b-47b9-485c-9873-0e815743db75"
      },
      "source": [
        "network_NICER_KLML = invnet(double_well.dim, 'NNNNS', double_well, nl_layers=3, nl_hidden=100, \n",
        "                            nl_activation='relu', nl_activation_scale='tanh')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<NNNN>S\n",
            "< 2 0 0\n",
            "N 1 1 0\n",
            "N 1 1 0\n",
            "N 1 1 0\n",
            "N 1 1 0\n",
            "> 1 1 0\n",
            "S 2 0 0\n",
            "tracking <tf.Variable 'log_scale:0' shape=(1, 2) dtype=float32, numpy=array([[0., 0.]], dtype=float32)> log_scaling_factors\n",
            "tracking <tf.Variable 'log_scale:0' shape=(1, 2) dtype=float32, numpy=array([[0., 0.]], dtype=float32)> log_scaling_factors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s730etNcWlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#network_NICER_KLML.Tzx.summary()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb8-l9FYPMCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 50; batch_size = 1024\n",
        "(loss, samples) = network_NICER_KLML.train_KL(epochs=epochs, batch_size=batch_size, lr=0.001, temperature=1.0, verbose=0, explore=1.0)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLnAHannd2Za",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "9fca376e-6755-438e-ff4e-2ec30fbead7c"
      },
      "source": [
        "import seaborn as sns\n",
        "with sns.color_palette(\"cubehelix\",10):\n",
        "  for epoch in samples[::epochs//10]:\n",
        "    sns.distplot(epoch[:,0], hist=False)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_SE0VxyH4bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}