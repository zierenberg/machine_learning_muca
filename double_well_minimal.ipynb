{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "double_well_minimal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyORP47q/ROuIQpW4Oo9KYrj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zierenberg/machine_learning_muca/blob/MW/double_well_minimal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkbmlc-_ZtSA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "12c37aa7-eb16-4f7e-9392-2bebdffce47a"
      },
      "source": [
        "!git clone https://github.com/zierenberg/machine_learning_muca.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'machine_learning_muca'...\n",
            "remote: Enumerating objects: 66, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 66 (delta 11), reused 40 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (66/66), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WZMDn37Mx_W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5ccd71c1-1b71-4595-a2a8-5f30a327e714"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "import keras\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi2psdzKakPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DoubleWell(object):\n",
        "\n",
        "    params_default = {'a4' : 1.0,\n",
        "                      'a2' : 6.0,\n",
        "                      'a1' : 1.0,\n",
        "                      'k' : 1.0,\n",
        "                      'dim' : 1}\n",
        "\n",
        "    def __init__(self, params=None):\n",
        "        # set parameters\n",
        "        if params is None:\n",
        "            params = self.__class__.params_default\n",
        "        self.params = params\n",
        "\n",
        "        # useful variables\n",
        "        self.dim = self.params['dim']\n",
        "\n",
        "\n",
        "    def energy(self, x):\n",
        "        dimer_energy =self.params['a4'] * x[:, 0] ** 4 - self.params['a2'] * x[:, 0] ** 2 + self.params['a1'] * x[:, 0]\n",
        "        oscillator_energy = 0.0\n",
        "        if self.dim == 2:\n",
        "            oscillator_energy = (self.params['k'] / 2.0) * x[:, 1] ** 2\n",
        "        if self.dim > 2:\n",
        "            oscillator_energy = np.sum((self.params['k'] / 2.0) * x[:, 1:] ** 2, axis=1)\n",
        "        return  dimer_energy + oscillator_energy\n",
        "\n",
        "    def energy_tf(self, x):\n",
        "        dimer_energy =self.params['a4'] * x[:, 0] ** 4 - self.params['a2'] * x[:, 0] ** 2 + self.params['a1'] * x[:, 0]\n",
        "        oscillator_energy = 0.0\n",
        "        if self.dim == 2:\n",
        "            oscillator_energy = (self.params['k'] / 2.0) * x[:, 1] ** 2\n",
        "        if self.dim > 2:\n",
        "            oscillator_energy = tf.reduce_sum(input_tensor=(self.params['k'] / 2.0) * x[:, 1:] ** 2, axis=1)\n",
        "        return  dimer_energy + oscillator_energy\n",
        "\n",
        "    def plot_dimer_energy(self, axis=None, temperature=1.0):\n",
        "        \"\"\" Plots the dimer energy to the standard figure \"\"\"\n",
        "        x_grid = np.linspace(-3, 3, num=200)\n",
        "        if self.dim == 1:\n",
        "            X = x_grid[:, None]\n",
        "        else:\n",
        "            X = np.hstack([x_grid[:, None], np.zeros((x_grid.size, self.dim - 1))])\n",
        "        energies = self.energy(X) / temperature\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "        if axis is None:\n",
        "            axis = plt.gca()\n",
        "        #plt.figure(figsize=(5, 4))\n",
        "        axis.plot(x_grid, energies, linewidth=3, color='black')\n",
        "        axis.set_xlabel('x / a.u.')\n",
        "        axis.set_ylabel('Energy / kT')\n",
        "        axis.set_ylim(energies.min() - 2.0, energies[int(energies.size / 2)] + 2.0)\n",
        "\n",
        "        return x_grid, energies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7eITfCBbVTI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "adda976a-b142-4ffc-fd30-9df92c901b7d"
      },
      "source": [
        "params = DoubleWell.params_default.copy()\n",
        "params['dim'] =2\n",
        "double_well = DoubleWell(params=params)\n",
        "plt.figure(figsize=(5,5))\n",
        "double_well.plot_dimer_energy();"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPSIKMgabbiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MetropolisGauss(object):\n",
        "\n",
        "    def __init__(self, model, x0, temperature=1.0, noise=0.1,\n",
        "                 burnin=0, stride=1, nwalkers=1, mapper=None):\n",
        "        \"\"\" Metropolis Monte-Carlo Simulation with Gaussian Proposal Steps\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model : Energy model\n",
        "            Energy model object, must provide the function energy(x)\n",
        "        x0 : [array]\n",
        "            Initial configuration\n",
        "        noise : float\n",
        "            Noise intensity, standard deviation of Gaussian proposal step\n",
        "        temperatures : float or array\n",
        "            Temperature. By default (1.0) the energy is interpreted in reduced units.\n",
        "            When given an array, its length must correspond to nwalkers, then the walkers\n",
        "            are simulated at different temperatures.\n",
        "        burnin : int\n",
        "            Number of burn-in steps that will not be saved\n",
        "        stride : int\n",
        "            Every so many steps will be saved\n",
        "        nwalkers : int\n",
        "            Number of parallel walkers\n",
        "        mapper : Mapper object\n",
        "            Object with function map(X), e.g. to remove permutation.\n",
        "            If given will be applied to each accepted configuration.\n",
        "\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.noise = noise\n",
        "        self.temperature = temperature\n",
        "        self.burnin = burnin\n",
        "        self.stride = stride\n",
        "        self.nwalkers = nwalkers\n",
        "        if mapper is None:\n",
        "            class DummyMapper(object):\n",
        "                def map(self, X):\n",
        "                    return X\n",
        "            mapper = DummyMapper()\n",
        "        self.mapper = mapper\n",
        "        self.reset(x0)\n",
        "\n",
        "    def _proposal_step(self):\n",
        "        # proposal step\n",
        "        self.x_prop = self.x + self.noise*np.random.randn(self.x.shape[0], self.x.shape[1])\n",
        "        self.x_prop = self.mapper.map(self.x_prop)\n",
        "        self.E_prop = self.model.energy(self.x_prop)\n",
        "\n",
        "    def _acceptance_step(self):\n",
        "        # acceptance step\n",
        "        acc = -np.log(np.random.rand()) > (self.E_prop - self.E) / self.temperature\n",
        "        self.x = np.where(acc[:, None], self.x_prop, self.x)\n",
        "        self.E = np.where(acc, self.E_prop, self.E)\n",
        "\n",
        "    def reset(self, x0):\n",
        "        # counters\n",
        "        self.step = 0\n",
        "        self.traj_ = []\n",
        "        self.etraj_ = []\n",
        "\n",
        "        # initial configuration\n",
        "        self.x = np.tile(x0, (self.nwalkers, 1))\n",
        "        self.x = self.mapper.map(self.x)\n",
        "        self.E = self.model.energy(self.x)\n",
        "\n",
        "        # save first frame if no burnin\n",
        "        if self.burnin == 0:\n",
        "            self.traj_.append(self.x)\n",
        "            self.etraj_.append(self.E)\n",
        "\n",
        "    @property\n",
        "    def trajs(self):\n",
        "        \"\"\" Returns a list of trajectories, one trajectory for each walker \"\"\"\n",
        "        T = np.array(self.traj_).astype(np.float32)\n",
        "        return [T[:, i, :] for i in range(T.shape[1])]\n",
        "\n",
        "    @property\n",
        "    def traj(self):\n",
        "        return self.trajs[0]\n",
        "\n",
        "    @property\n",
        "    def etrajs(self):\n",
        "        \"\"\" Returns a list of energy trajectories, one trajectory for each walker \"\"\"\n",
        "        E = np.array(self.etraj_)\n",
        "        return [E[:, i] for i in range(E.shape[1])]\n",
        "\n",
        "    @property\n",
        "    def etraj(self):\n",
        "        return self.etrajs[0]\n",
        "\n",
        "    def run(self, nsteps=1, verbose=0):\n",
        "        for i in range(nsteps):\n",
        "            self._proposal_step()\n",
        "            self._acceptance_step()\n",
        "            self.step += 1\n",
        "            if verbose > 0 and i % verbose == 0:\n",
        "                print('Step', i, '/', nsteps)\n",
        "            if self.step > self.burnin and self.step % self.stride == 0:\n",
        "                self.traj_.append(self.x)\n",
        "                self.etraj_.append(self.E)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raqT2XoaeHgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nsteps = 10000\n",
        "x0_left = np.array([[-1.8, 0.0]])\n",
        "x0_right = np.array([[1.8, 0.0]])\n",
        "\n",
        "sampler = MetropolisGauss(double_well, x0_left, noise=0.1, stride=10)\n",
        "sampler.run(nsteps)\n",
        "traj_left = sampler.traj.copy()\n",
        "\n",
        "sampler.reset(x0_left)\n",
        "sampler.run(nsteps)\n",
        "traj_left_val = sampler.traj.copy()\n",
        "\n",
        "sampler.reset(x0_right)\n",
        "sampler.run(nsteps)\n",
        "traj_right = sampler.traj.copy()\n",
        "\n",
        "sampler.reset(x0_right)\n",
        "sampler.run(nsteps)\n",
        "traj_right_val = sampler.traj.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1X_c42pJSwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "1874fdb4-5d19-4543-cfb8-dccc08a69e2a"
      },
      "source": [
        "plt.figure(figsize=(9, 4))\n",
        "ax1 = plt.subplot2grid((1, 3), (0, 0), colspan=2)\n",
        "ax2 = plt.subplot2grid((1, 3), (0, 2))\n",
        "ax1.plot(traj_left[:, 0], color='blue', alpha=0.7)\n",
        "ax1.plot(traj_right[:, 0], color='red', alpha=0.7)\n",
        "ax1.set_xlim(0, 1000)\n",
        "ax1.set_ylim(-2.5, 2.5)\n",
        "ax1.set_xlabel('Time / steps')\n",
        "ax1.set_ylabel('$x_1$ / a.u.')\n",
        "ax2.hist(traj_left[:, 0], 30, orientation='horizontal', histtype='stepfilled', color='blue', alpha=0.2);\n",
        "ax2.hist(traj_left[:, 0], 30, orientation='horizontal', histtype='step', color='blue', linewidth=2);\n",
        "ax2.hist(traj_right[:, 0], 30, orientation='horizontal', histtype='stepfilled', color='red', alpha=0.2);\n",
        "ax2.hist(traj_right[:, 0], 30, orientation='horizontal', histtype='step', color='red', linewidth=2);\n",
        "ax2.set_xticks([])\n",
        "ax2.set_yticks([])\n",
        "ax2.set_ylim(-2.5, 2.5)\n",
        "ax2.set_xlabel('Probability')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Probability')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWZgO2a9SpUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.vstack([traj_left, traj_right])\n",
        "xval = np.vstack([traj_left_val, traj_right_val])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvlEvEwARab-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# util.py\n",
        "\n",
        "def ensure_traj(X):\n",
        "    if np.ndim(X) == 2:\n",
        "        return X\n",
        "    if np.ndim(X) == 1:\n",
        "        return np.array([X])\n",
        "    raise ValueError('Incompatible array with shape: ', np.shape(X))\n",
        "\n",
        "def connect(input_layer, layers):\n",
        "    \"\"\" Connect the given sequence of layers and returns output layer\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_layer : keras layer\n",
        "        Input layer\n",
        "    layers : list of keras layers\n",
        "        Layers to be connected sequentially\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    output_layer : kears layer\n",
        "        Output Layer\n",
        "\n",
        "    \"\"\"\n",
        "    layer = input_layer\n",
        "    for l in layers:\n",
        "        layer = l(layer)\n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUbt_dM0P2Yz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# layers_basic.py\n",
        "\n",
        "import numbers\n",
        "\n",
        "def nonlinear_transform(output_size, nlayers=3, nhidden=100, activation='relu', init_outputs=None, **args):\n",
        "    \"\"\" Generic dense trainable nonlinear transform\n",
        "\n",
        "    Returns the layers of a dense feedforward network with nlayers-1 hidden layers with nhidden neurons\n",
        "    and the specified activation functions. The last layer is linear in order to access the full real\n",
        "    number range and has output_size output neurons.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    output_size : int\n",
        "        number of output neurons\n",
        "    nlayers : int\n",
        "        number of layers, including the linear output layer. nlayers=3 means two hidden layers with\n",
        "        nonlinear activation and one linear output layer.\n",
        "    nhidden : int\n",
        "        number of neurons in each hidden layer, either a number or an array of length nlayers-1\n",
        "        to specify the width of each hidden layer\n",
        "    activation : str\n",
        "        nonlinear activation function in hidden layers\n",
        "    init_outputs : None or float or array\n",
        "        None means default initialization for the output layer, otherwise it is currently initialized with 0\n",
        "    **args : kwargs\n",
        "        Additional keyword arguments passed to the layer\n",
        "\n",
        "    \"\"\"\n",
        "    if isinstance(nhidden, numbers.Integral):\n",
        "        nhidden = nhidden * np.ones(nlayers-1, dtype=int)\n",
        "    else:\n",
        "        nhidden = np.array(nhidden)\n",
        "        if nhidden.size != nlayers-1:\n",
        "            raise ValueError('Illegal size of nhidden. Expecting 1d array with nlayers-1 elements')\n",
        "    M = [keras.layers.Dense(nh, activation=activation, **args) for nh in nhidden]\n",
        "    if init_outputs is None:\n",
        "        final_layer = keras.layers.Dense(output_size, activation='linear', **args)\n",
        "    else:\n",
        "        argscopy = copy.deepcopy(args)\n",
        "        argscopy['kernel_initializer'] = keras.initializers.Zeros()\n",
        "        argscopy['bias_initializer'] = keras.initializers.Constant(init_outputs)\n",
        "        final_layer = keras.layers.Dense(output_size, activation='linear', **argscopy)\n",
        "                                         #kernel_initializer=keras.initializers.Zeros(),\n",
        "                                         #bias_initializer=keras.initializers.Constant(init_outputs))\n",
        "    M += [final_layer]\n",
        "\n",
        "    return M\n",
        "\n",
        "\n",
        "class ResampleLayer(keras.engine.Layer):\n",
        "    \"\"\"\n",
        "    Receives as inputs latent space encodings z and normal noise w. Transforms w to\n",
        "    Match the mean and the standard deviations of z.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, **kwargs):\n",
        "        self.dim = dim\n",
        "        super(ResampleLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, x):\n",
        "        # split input into latent and noise variables\n",
        "        z = x[:, :self.dim]\n",
        "        w = x[:, self.dim:]\n",
        "        #z, w = x\n",
        "        # mean\n",
        "        mean = keras.backend.mean(z, axis=0)\n",
        "        # covariance matrix\n",
        "        batchsize = keras.backend.shape(z)[0]\n",
        "        cov = keras.backend.dot(keras.backend.transpose(z), z) / keras.backend.cast(batchsize, np.float32)\n",
        "        # standard deviations\n",
        "        std = tf.sqrt(tf.linalg.tensor_diag_part(cov))\n",
        "        # transform w and return\n",
        "        wtrans = tf.reshape(mean, (1, self.dim)) + w * tf.reshape(std, (1, self.dim))\n",
        "        return wtrans\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], self.dim\n",
        "\n",
        "\n",
        "class IndexLayer(keras.engine.Layer):\n",
        "    def __init__(self, indices, **kwargs):\n",
        "        \"\"\" Returns [:, indices].\n",
        "        \"\"\"\n",
        "        self.indices = indices\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, x):\n",
        "        # split input\n",
        "        return tf.gather(x, self.indices, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], self.indices.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FhbACMiLwrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# invertible_layers.py\n",
        "\n",
        "def split_merge_indices(ndim, nchannels=2, channels=None):\n",
        "    if channels is None:\n",
        "        channels = np.tile(np.arange(nchannels), int(ndim/nchannels)+1)[:ndim]\n",
        "    else:\n",
        "        channels = np.array(channels)\n",
        "        nchannels = np.max(channels) + 1\n",
        "    indices_split = []\n",
        "    idx = np.arange(ndim)\n",
        "    for c in range(nchannels):\n",
        "        isplit = np.where(channels == c)[0]\n",
        "        indices_split.append(isplit)\n",
        "    indices_merge = np.concatenate(indices_split).argsort()\n",
        "    return channels, indices_split, indices_merge\n",
        "\n",
        "class Permute(object):\n",
        "    def __init__(self, ndim, order=None):\n",
        "        \"\"\" Permutes dimensions\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        order : None or array\n",
        "            If None, a random permutation will be chosen.\n",
        "            Otherwise, specify the new order of dimensions for x -> z.\n",
        "\n",
        "        \"\"\"\n",
        "        self.ndim = ndim\n",
        "        if order is None:\n",
        "            order = np.random.choice(ndim, ndim, replace=False)\n",
        "        self.order = order\n",
        "        self.reverse = np.argsort(order)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, D):\n",
        "        ndim = D['ndim']\n",
        "        order = D['order']\n",
        "        return cls(ndim, order=order)\n",
        "\n",
        "    def to_dict(self):\n",
        "        D = {}\n",
        "        D['ndim'] = self.ndim\n",
        "        D['order'] = self.order\n",
        "        return D\n",
        "\n",
        "    def connect_xz(self, x):\n",
        "        self.output_z = IndexLayer(self.order)(x)\n",
        "        return self.output_z\n",
        "\n",
        "    def connect_zx(self, z):\n",
        "        self.output_x = IndexLayer(self.reverse)(z)\n",
        "        return self.output_x\n",
        "\n",
        "\n",
        "class SplitChannels(object):\n",
        "    def __init__(self, ndim, nchannels=2, channels=None):\n",
        "        \"\"\" Splits channels forward and merges them backward \"\"\"\n",
        "        self.channels, self.indices_split, self.indices_merge = split_merge_indices(ndim, nchannels=nchannels,\n",
        "                                                                                    channels=channels)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, D):\n",
        "        channels = D['channels']\n",
        "        dim = channels.size\n",
        "        nchannels = channels.max() + 1\n",
        "        return cls(dim, nchannels=nchannels, channels=channels)\n",
        "\n",
        "    def to_dict(self):\n",
        "        D = {}\n",
        "        D['channels'] = self.channels\n",
        "        return D\n",
        "\n",
        "    def connect_xz(self, x):\n",
        "        # split X into different coordinate channels\n",
        "        self.output_z = [IndexLayer(isplit)(x) for isplit in self.indices_split]\n",
        "        return self.output_z\n",
        "\n",
        "    def connect_zx(self, z):\n",
        "        # first concatenate\n",
        "        x_scrambled = keras.layers.Concatenate()(z)\n",
        "        # unscramble x\n",
        "        self.output_x = IndexLayer(self.indices_merge)(x_scrambled) # , name='output_x'\n",
        "        return self.output_x\n",
        "\n",
        "\n",
        "class MergeChannels(SplitChannels):\n",
        "    def connect_xz(self, x):\n",
        "        # first concatenate\n",
        "        z_scrambled = keras.layers.Concatenate()(x)\n",
        "        # unscramble x\n",
        "        self.output_z = IndexLayer(self.indices_merge)(z_scrambled) # , name='output_z'\n",
        "        return self.output_z\n",
        "\n",
        "    def connect_zx(self, z):\n",
        "        # split X into different coordinate channels\n",
        "        self.output_x = [IndexLayer(isplit)(z) for isplit in self.indices_split]\n",
        "        return self.output_x\n",
        "\n",
        "\n",
        "class Scaling(object):\n",
        "    def __init__(self, ndim, scaling_factors=None, trainable=True, name_xz=None, name_zx=None):\n",
        "        \"\"\" Invertible Scaling layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ndim : int\n",
        "            Number of dimensions\n",
        "        scaling_factors : array\n",
        "            Initial scaling factors, must be of shape (1, ndim)\n",
        "        trainable : bool\n",
        "            If True, scaling factors are trainable. If false, they are fixed\n",
        "        name_xz : str\n",
        "            Name for Sxz\n",
        "        name_xz : str\n",
        "            Name for Szx\n",
        "\n",
        "        \"\"\"\n",
        "        # define local classes\n",
        "        class ScalingLayer(keras.engine.Layer):\n",
        "            def __init__(self, log_scaling_factors, **kwargs):\n",
        "                \"\"\" Layer that scales dimensions with trainable factors\n",
        "\n",
        "                Parameters\n",
        "                ----------\n",
        "                scaling_factors : (1xd) array\n",
        "                    scaling factors applied to columns of batch matrix.\n",
        "\n",
        "                \"\"\"\n",
        "                self.log_scaling_factors = log_scaling_factors\n",
        "                super().__init__(**kwargs)\n",
        "\n",
        "            def build(self, input_shape):\n",
        "                # Make weight trainable\n",
        "                if self.trainable:\n",
        "                    self._trainable_weights.append(self.log_scaling_factors)\n",
        "                super().build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "            def compute_output_shape(self, input_shape):\n",
        "                return (input_shape[0], self.log_scaling_factors.shape[1])\n",
        "\n",
        "        class ScalingXZ(ScalingLayer):\n",
        "            def __init__(self, log_scaling_factors, **kwargs):\n",
        "                \"\"\" Layer that scales the batch X in (B,d) by X * S where S=diag(s1,...,sd)\n",
        "                \"\"\"\n",
        "                super().__init__(log_scaling_factors, **kwargs)\n",
        "\n",
        "            def call(self, x):\n",
        "                return x * tf.exp(self.log_scaling_factors)\n",
        "\n",
        "        class ScalingZX(ScalingLayer):\n",
        "            def __init__(self, log_scaling_factors, **kwargs):\n",
        "                \"\"\" Layer that scales the batch X in (B,d) by X * S^(-1) where S=diag(s1,...,sd)\n",
        "                \"\"\"\n",
        "                super().__init__(log_scaling_factors, **kwargs)\n",
        "\n",
        "            def call(self, x):\n",
        "                return x * tf.exp(-self.log_scaling_factors)\n",
        "\n",
        "        # initialize scaling factors\n",
        "        if scaling_factors is None:\n",
        "            self.log_scaling_factors = keras.backend.variable(np.zeros((1, ndim)),\n",
        "                                                              dtype=keras.backend.floatx(),\n",
        "                                                              name='log_scale')\n",
        "        else:\n",
        "            self.log_scaling_factors = keras.backend.variable(np.log(scaling_factors),\n",
        "                                                              dtype=keras.backend.floatx(),\n",
        "                                                              name='log_scale')\n",
        "\n",
        "        self.trainable = trainable\n",
        "        self.Sxz = ScalingXZ(self.log_scaling_factors, trainable=trainable, name=name_xz)\n",
        "        self.Szx = ScalingZX(self.log_scaling_factors, trainable=trainable, name=name_zx)\n",
        "\n",
        "    @property\n",
        "    def scaling_factors(self):\n",
        "        return tf.exp(self.log_scaling_factors)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, D):\n",
        "        scaling_factors = D['scaling_factors']\n",
        "        dim = scaling_factors.shape[1]\n",
        "        trainable = D['trainable']\n",
        "        name_xz = D['name_xz']\n",
        "        name_zx = D['name_zx']\n",
        "        return Scaling(dim, scaling_factors=scaling_factors, trainable=trainable, name_xz=name_xz, name_zx=name_zx)\n",
        "\n",
        "    def to_dict(self):\n",
        "        D = {}\n",
        "        D['scaling_factors'] = keras.backend.eval(self.scaling_factors)\n",
        "        D['trainable'] = self.trainable\n",
        "        D['name_xz'] = self.Sxz.name\n",
        "        D['name_zx'] = self.Szx.name\n",
        "        return D\n",
        "\n",
        "    def connect_xz(self, x):\n",
        "        def lambda_Jxz(x):\n",
        "            J = tf.reduce_sum(input_tensor=self.log_scaling_factors, axis=1)[0]\n",
        "            return J * keras.backend.ones((tf.shape(input=x)[0], 1))\n",
        "        self.log_det_xz = keras.layers.Lambda(lambda_Jxz)(x)\n",
        "        z = self.Sxz(x)\n",
        "        return z\n",
        "\n",
        "    def connect_zx(self, z):\n",
        "        def lambda_Jzx(x):\n",
        "            J = tf.reduce_sum(input_tensor=-self.log_scaling_factors, axis=1)[0]\n",
        "            return J * keras.backend.ones((tf.shape(input=x)[0], 1))\n",
        "        self.log_det_zx = keras.layers.Lambda(lambda_Jzx)(z)\n",
        "        x = self.Szx(z)\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def log_det_Jxz(self):\n",
        "        \"\"\" Log of |det(dz/dx)| for the current batch. Format is batchsize x 1 or a number \"\"\"\n",
        "        return self.log_det_xz\n",
        "\n",
        "    @property\n",
        "    def log_det_Jzx(self):\n",
        "        \"\"\" Log of |det(dx/dz)| for the current batch. Format is batchsize x 1 or a number \"\"\"\n",
        "        return self.log_det_zx\n",
        "\n",
        "\n",
        "class CompositeLayer(object):\n",
        "    def __init__(self, transforms):\n",
        "        \"\"\" Composite layer consisting of multiple keras layers with shared parameters  \"\"\"\n",
        "        self.transforms = transforms\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, d):\n",
        "        from deep_boltzmann.networks.util import deserialize_layers\n",
        "        transforms = deserialize_layers(d['transforms'])\n",
        "        return cls(transforms)\n",
        "\n",
        "    def to_dict(self):\n",
        "        from deep_boltzmann.networks.util import serialize_layers\n",
        "        D = {}\n",
        "        D['transforms'] = serialize_layers(self.transforms)\n",
        "        return D\n",
        "\n",
        "\n",
        "class NICER(CompositeLayer):\n",
        "    def __init__(self, transforms):\n",
        "        \"\"\" Two sequential NICE transformations and their inverse transformations.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        transforms : list\n",
        "            List with [M1, M2] containing the keras layers for nonlinear transformation 1 and 2.\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__(transforms)\n",
        "        self.M1 = transforms[0]\n",
        "        self.M2 = transforms[1]\n",
        "\n",
        "    def connect_xz(self, x):\n",
        "        x1 = x[0]\n",
        "        x2 = x[1]\n",
        "        self.input_x1 = x1\n",
        "        self.input_x2 = x2\n",
        "\n",
        "        # first stage backward\n",
        "        y2 = x2\n",
        "        y1 = keras.layers.Subtract()([x1, connect(x2, self.M2)])\n",
        "        # second stage backward\n",
        "        z1 = y1\n",
        "        z2 = keras.layers.Subtract()([y2, connect(y1, self.M1)])\n",
        "\n",
        "        return [z1, z2] + x[2:]  # append other layers if there are any\n",
        "\n",
        "    def connect_zx(self, z):\n",
        "        z1 = z[0]\n",
        "        z2 = z[1]\n",
        "        self.input_z1 = z1\n",
        "        self.input_z2 = z2\n",
        "\n",
        "        # first stage forward\n",
        "        y1 = z1\n",
        "        y2 = keras.layers.Add()([z2, connect(z1, self.M1)])\n",
        "        # second stage forward\n",
        "        x2 = y2\n",
        "        x1 = keras.layers.Add()([y1, connect(y2, self.M2)])\n",
        "\n",
        "        return [x1, x2] + z[2:]  # append other layers if there are any\n",
        "\n",
        "\n",
        "class RealNVP(CompositeLayer):\n",
        "    def __init__(self, transforms):\n",
        "        \"\"\" Two sequential NVP transformations and their inverse transformatinos.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        transforms : list\n",
        "            List [S1, T1, S2, T2] with keras layers for scaling and translation transforms\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__(transforms)\n",
        "        self.S1 = transforms[0]\n",
        "        self.T1 = transforms[1]\n",
        "        self.S2 = transforms[2]\n",
        "        self.T2 = transforms[3]\n",
        "\n",
        "    def connect_xz(self, x):\n",
        "        def lambda_exp(x):\n",
        "            return keras.backend.exp(x)\n",
        "        def lambda_sum(x):\n",
        "            return keras.backend.sum(x[0], axis=1, keepdims=True) + keras.backend.sum(x[1], axis=1, keepdims=True)\n",
        "\n",
        "        x1 = x[0]\n",
        "        x2 = x[1]\n",
        "        self.input_x1 = x1\n",
        "        self.input_x2 = x2\n",
        "\n",
        "        y1 = x1\n",
        "        self.Sxy_layer = connect(x1, self.S1)\n",
        "        self.Txy_layer = connect(x1, self.T1)\n",
        "        prodx = keras.layers.Multiply()([x2, keras.layers.Lambda(lambda_exp)(self.Sxy_layer)])\n",
        "        y2 = keras.layers.Add()([prodx, self.Txy_layer])\n",
        "\n",
        "        self.output_z2 = y2\n",
        "        self.Syz_layer = connect(y2, self.S2)\n",
        "        self.Tyz_layer = connect(y2, self.T2)\n",
        "        prody = keras.layers.Multiply()([y1, keras.layers.Lambda(lambda_exp)(self.Syz_layer)])\n",
        "        self.output_z1 = keras.layers.Add()([prody, self.Tyz_layer])\n",
        "\n",
        "        # log det(dz/dx)\n",
        "        self.log_det_xz = keras.layers.Lambda(lambda_sum)([self.Sxy_layer, self.Syz_layer])\n",
        "\n",
        "        return [self.output_z1, self.output_z2] + x[2:]  # append other layers if there are any\n",
        "\n",
        "    def connect_zx(self, z):\n",
        "        def lambda_negexp(x):\n",
        "            return keras.backend.exp(-x)\n",
        "        def lambda_negsum(x):\n",
        "            return keras.backend.sum(-x[0], axis=1, keepdims=True) + keras.backend.sum(-x[1], axis=1, keepdims=True)\n",
        "\n",
        "        z1 = z[0]\n",
        "        z2 = z[1]\n",
        "        self.input_z1 = z1\n",
        "        self.input_z2 = z2\n",
        "\n",
        "        y2 = z2\n",
        "        self.Szy_layer = connect(z2, self.S2)\n",
        "        self.Tzy_layer = connect(z2, self.T2)\n",
        "        z1_m_Tz2 = keras.layers.Subtract()([z1, self.Tzy_layer])\n",
        "        y1 = keras.layers.Multiply()([z1_m_Tz2, keras.layers.Lambda(lambda_negexp)(self.Szy_layer)])\n",
        "\n",
        "        self.output_x1 = y1\n",
        "        self.Syx_layer = connect(y1, self.S1)\n",
        "        self.Tyx_layer = connect(y1, self.T1)\n",
        "        y2_m_Ty1 = keras.layers.Subtract()([y2, self.Tyx_layer])\n",
        "        self.output_x2 = keras.layers.Multiply()([y2_m_Ty1, keras.layers.Lambda(lambda_negexp)(self.Syx_layer)])\n",
        "\n",
        "        # log det(dx/dz)\n",
        "        # TODO: check Jacobian\n",
        "        self.log_det_zx = keras.layers.Lambda(lambda_negsum)([self.Szy_layer, self.Syx_layer])\n",
        "\n",
        "        return [self.output_x1, self.output_x2] + z[2:]  # append other layers if there are any\n",
        "\n",
        "    @property\n",
        "    def log_det_Jxz(self):\n",
        "        \"\"\" Log of |det(dz/dx)| for the current batch. Format is batchsize x 1 or a number \"\"\"\n",
        "        return self.log_det_xz\n",
        "\n",
        "    @property\n",
        "    def log_det_Jzx(self):\n",
        "        \"\"\" Log of |det(dx/dz)| for the current batch. Format is batchsize x 1 or a number \"\"\"\n",
        "        return self.log_det_zx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh0Regw_JjPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# invertible.py\n",
        "\n",
        "class InvNet(object):\n",
        "\n",
        "    def __init__(self, dim, layers, prior='normal'):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        dim : int\n",
        "            Dimension\n",
        "        layers : list\n",
        "            list of invertible layers\n",
        "        prior : str\n",
        "            Type of prior, 'normal', 'lognormal'\n",
        "\n",
        "        \"\"\"\n",
        "        \"\"\" Stack of invertible layers \"\"\"\n",
        "        self.dim = dim\n",
        "        self.layers = layers\n",
        "        self.prior = prior\n",
        "        self.connect_layers()\n",
        "        # compute total Jacobian for x->z transformation\n",
        "        log_det_xzs = []\n",
        "        for l in layers:\n",
        "            if hasattr(l, 'log_det_xz'):\n",
        "                log_det_xzs.append(l.log_det_xz)\n",
        "        if len(log_det_xzs) == 0:\n",
        "            self.TxzJ = None\n",
        "        else:\n",
        "            if len(log_det_xzs) == 1:\n",
        "                self.log_det_xz = log_det_xzs[0]\n",
        "            else:\n",
        "                self.log_det_xz = keras.layers.Add()(log_det_xzs)\n",
        "            self.TxzJ = keras.models.Model(inputs=self.input_x, outputs=[self.output_z, self.log_det_xz])\n",
        "        # compute total Jacobian for z->x transformation\n",
        "        log_det_zxs = []\n",
        "        for l in layers:\n",
        "            if hasattr(l, 'log_det_zx'):\n",
        "                log_det_zxs.append(l.log_det_zx)\n",
        "        if len(log_det_zxs) == 0:\n",
        "            self.TzxJ = None\n",
        "        else:\n",
        "            if len(log_det_zxs) == 1:\n",
        "                self.log_det_zx = log_det_zxs[0]\n",
        "            else:\n",
        "                self.log_det_zx = keras.layers.Add()(log_det_zxs)\n",
        "            self.TzxJ = keras.models.Model(inputs=self.input_z, outputs=[self.output_x, self.log_det_zx])\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, filename, clear_session=True):\n",
        "        \"\"\" Loads parameters into model. Careful: this clears the whole TF session!!\n",
        "        \"\"\"\n",
        "        from deep_boltzmann.util import load_obj\n",
        "        if clear_session:\n",
        "            keras.backend.clear_session()\n",
        "        D = load_obj(filename)\n",
        "        prior = D['prior']\n",
        "        layerdicts = D['layers']\n",
        "        layers = [eval(d['type']).from_dict(d) for d in layerdicts]\n",
        "        return InvNet(D['dim'], layers, prior=prior)\n",
        "\n",
        "    def save(self, filename):\n",
        "        from deep_boltzmann.util import save_obj\n",
        "        D = {}\n",
        "        D['dim'] = self.dim\n",
        "        D['prior'] = self.prior\n",
        "        layerdicts = []\n",
        "        for l in self.layers:\n",
        "            d = l.to_dict()\n",
        "            d['type'] = l.__class__.__name__\n",
        "            layerdicts.append(d)\n",
        "        D['layers'] = layerdicts\n",
        "        save_obj(D, filename)\n",
        "\n",
        "    def connect_xz(self, x):\n",
        "        z = None\n",
        "        for i in range(len(self.layers)):\n",
        "            z = self.layers[i].connect_xz(x)  # connect\n",
        "            #print(self.layers[i])\n",
        "            #print('Inputs\\n', x)\n",
        "            #print()\n",
        "            #print('Outputs\\n', z)\n",
        "            #print('------------')\n",
        "            #print()\n",
        "            x = z  # rename output\n",
        "        return z\n",
        "\n",
        "    def connect_zx(self, z):\n",
        "        x = None\n",
        "        for i in range(len(self.layers)-1, -1, -1):\n",
        "            x = self.layers[i].connect_zx(z)  # connect\n",
        "            #print(self.layers[i])\n",
        "            #print('Inputs\\n', z)\n",
        "            #print()\n",
        "            #print('Outputs\\n', x)\n",
        "            #print('------------')\n",
        "            #print()\n",
        "            z = x  # rename output to next input\n",
        "        return x\n",
        "\n",
        "    def connect_layers(self):\n",
        "        # X -> Z\n",
        "        self.input_x = keras.layers.Input(shape=(self.dim,))\n",
        "        self.output_z = self.connect_xz(self.input_x)\n",
        "\n",
        "        # Z -> X\n",
        "        self.input_z = keras.layers.Input(shape=(self.dim,))\n",
        "        self.output_x = self.connect_zx(self.input_z)\n",
        "\n",
        "        # build networks\n",
        "        self.Txz = keras.models.Model(inputs=self.input_x, outputs=self.output_z)\n",
        "        self.Tzx = keras.models.Model(inputs=self.input_z, outputs=self.output_x)\n",
        "\n",
        "    def predict_log_det_Jxz(self, z):\n",
        "        if self.TzxJ is None:\n",
        "            return np.ones(z.shape[0])\n",
        "        else:\n",
        "            return self.TzxJ.predict(z)[1][:, 0]\n",
        "\n",
        "    @property\n",
        "    def log_det_Jxz(self):\n",
        "        \"\"\" Log of |det(dz/dx)| for the current batch. Format is batchsize x 1 or a number \"\"\"\n",
        "        #return self.log_det_xz.output\n",
        "        log_det_Jxzs = []\n",
        "        for l in self.layers:\n",
        "            if hasattr(l, 'log_det_Jxz'):\n",
        "                log_det_Jxzs.append(l.log_det_Jxz)\n",
        "        if len(log_det_Jxzs) == 0:\n",
        "            return tf.ones((self.output_z.shape[0],))\n",
        "        if len(log_det_Jxzs) == 1:\n",
        "            return log_det_Jxzs[0]\n",
        "        return tf.reduce_sum(input_tensor=log_det_Jxzs, axis=0, keepdims=False)\n",
        "\n",
        "    @property\n",
        "    def log_det_Jzx(self):\n",
        "        \"\"\" Log of |det(dx/dz)| for the current batch. Format is batchsize x 1 or a number \"\"\"\n",
        "        #return self.log_det_zx.output\n",
        "        log_det_Jzxs = []\n",
        "        for l in self.layers:\n",
        "            if hasattr(l, 'log_det_Jzx'):\n",
        "                log_det_Jzxs.append(l.log_det_Jzx)\n",
        "        if len(log_det_Jzxs) == 0:\n",
        "            return tf.ones((self.output_x.shape[0],))\n",
        "        if len(log_det_Jzxs) == 1:\n",
        "            return log_det_Jzxs[0]\n",
        "        return tf.reduce_sum(input_tensor=log_det_Jzxs, axis=0, keepdims=False)\n",
        "\n",
        "    def log_likelihood_z_normal(self, std=1.0):\n",
        "        \"\"\" Returns the log likelihood of z|x assuming a Normal distribution in z\n",
        "        \"\"\"\n",
        "        #return self.log_det_Jxz - self.dim * tf.log(std) - (0.5 / (std**2)) * tf.reduce_sum(self.output_z**2, axis=1)\n",
        "        return self.log_det_Jxz - (0.5 / (std**2)) * tf.reduce_sum(input_tensor=self.output_z**2, axis=1)\n",
        "\n",
        "    def log_likelihood_z_lognormal(self, std=1.0):\n",
        "        \"\"\" Returns the log likelihood of z|x assuming a Normal distribution in z\n",
        "        \"\"\"\n",
        "        #return self.log_det_Jxz - self.dim * tf.log(std) - (0.5 / (std**2)) * tf.reduce_sum(self.output_z**2, axis=1)\n",
        "        from deep_boltzmann.util import logreg\n",
        "        logz = logreg(self.output_z, a=0.001, tf=True)\n",
        "        ll = self.log_det_Jxz \\\n",
        "             - (0.5 / (std**2)) * tf.reduce_sum(input_tensor=logz**2, axis=1) \\\n",
        "             - tf.reduce_sum(input_tensor=logz, axis=1)\n",
        "        return ll\n",
        "\n",
        "    def log_likelihood_z_cauchy(self, scale=1.0):\n",
        "        return -tf.reduce_sum(input_tensor=tf.math.log(1 + (self.output_z / scale)**2), axis=1)\n",
        "\n",
        "    def rc_entropy(self, rc_func, gmeans, gstd, ntemperatures=1):\n",
        "        \"\"\" Computes the entropy along a 1D reaction coordinate\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        rc_func : function\n",
        "            function to compute reaction coordinate\n",
        "        gmeans : array\n",
        "            mean positions of Gauss kernels along reaction coordinate\n",
        "        gstd : float\n",
        "            standard deviation of Gauss kernels along reaction coordinate\n",
        "        \"\"\"\n",
        "        # evaluate rc\n",
        "        rc = rc_func(self.output_x)\n",
        "        rc = tf.expand_dims(rc, axis=1)\n",
        "        # kernelize all values\n",
        "        kmat = tf.exp(-((rc - gmeans)**2) / (2*gstd*gstd))\n",
        "        kmat += 1e-6\n",
        "        kmat /= tf.reduce_sum(input_tensor=kmat, axis=1, keepdims=True)\n",
        "        # distribute counts across temperatures\n",
        "        batchsize_per_temperature = tf.cast(tf.shape(input=kmat)[0] / ntemperatures, tf.int32)\n",
        "        nbins = tf.shape(input=gmeans)[0]\n",
        "        kmatT = tf.transpose(a=tf.reshape(kmat, (batchsize_per_temperature, ntemperatures, nbins)), perm=(1, 0, 2))\n",
        "        histogram = tf.reduce_mean(input_tensor=kmatT, axis=1)\n",
        "        entropies = tf.reduce_sum(input_tensor=tf.math.log(histogram), axis=1)\n",
        "        return tf.reduce_mean(input_tensor=entropies)\n",
        "\n",
        "    def reg_Jzx_uniform(self):\n",
        "        \"\"\" Returns the log likelihood of z|x assuming a Normal distribution in z\n",
        "        \"\"\"\n",
        "        #return self.log_det_Jxz - self.dim * tf.log(std) - (0.5 / (std**2)) * tf.reduce_sum(self.output_z**2, axis=1)\n",
        "        Jmean = tf.reduce_mean(input_tensor=self.log_det_Jzx, axis=0, keepdims=True)\n",
        "        Jdev = tf.reduce_mean(input_tensor=(self.log_det_Jzx - Jmean) ** 2, axis=1, keepdims=False)\n",
        "        return Jdev\n",
        "\n",
        "    def reg_Jxz_uniform(self):\n",
        "        \"\"\" Returns the log likelihood of z|x assuming a Normal distribution in z\n",
        "        \"\"\"\n",
        "        #return self.log_det_Jxz - self.dim * tf.log(std) - (0.5 / (std**2)) * tf.reduce_sum(self.output_z**2, axis=1)\n",
        "        Jmean = tf.reduce_mean(input_tensor=self.log_det_Jxz, axis=0, keepdims=True)\n",
        "        Jdev = tf.reduce_mean(input_tensor=(self.log_det_Jxz - Jmean) ** 2, axis=1, keepdims=False)\n",
        "        return Jdev\n",
        "\n",
        "    def log_likelihood_z_normal_2trajs(self, trajlength, std=1.0):\n",
        "        \"\"\" Returns the log of the sum of two trajectory likelihoods\n",
        "        \"\"\"\n",
        "        #return self.log_det_Jxz - self.dim * tf.log(std) - (0.5 / (std**2)) * tf.reduce_sum(self.output_z**2, axis=1)\n",
        "        J = self.log_det_Jxz\n",
        "        LL1 = tf.reduce_mean(input_tensor=J[:trajlength] - (0.5 / (std**2)) * tf.reduce_sum(input_tensor=self.output_z[:trajlength]**2, axis=1))\n",
        "        LL2 = tf.reduce_mean(input_tensor=J[trajlength:] - (0.5 / (std**2)) * tf.reduce_sum(input_tensor=self.output_z[trajlength:]**2, axis=1))\n",
        "        return tf.reduce_logsumexp(input_tensor=[LL1, LL2])\n",
        "\n",
        "    def train_ML(self, x, xval=None, optimizer=None, lr=0.001, clipnorm=None, epochs=2000, batch_size=1024,\n",
        "                 std=1.0, reg_Jxz=0.0, verbose=1, return_test_energies=False):\n",
        "        if optimizer is None:\n",
        "            if clipnorm is None:\n",
        "                optimizer = keras.optimizers.adam(lr=lr)\n",
        "            else:\n",
        "                optimizer = keras.optimizers.adam(lr=lr, clipnorm=clipnorm)\n",
        "\n",
        "        def loss_ML_normal(y_true, y_pred):\n",
        "            return -self.log_likelihood_z_normal(std=std)\n",
        "        def loss_ML_lognormal(y_true, y_pred):\n",
        "            return -self.log_likelihood_z_lognormal(std=std)\n",
        "        def loss_ML_cauchy(y_true, y_pred):\n",
        "            return -self.log_likelihood_z_cauchy(scale=std)\n",
        "        def loss_ML_normal_reg(y_true, y_pred):\n",
        "            return -self.log_likelihood_z_normal(std=std) + reg_Jxz*self.reg_Jxz_uniform()\n",
        "        def loss_ML_lognormal_reg(y_true, y_pred):\n",
        "            return -self.log_likelihood_z_lognormal(std=std) + reg_Jxz*self.reg_Jxz_uniform()\n",
        "        def loss_ML_cauchy_reg(y_true, y_pred):\n",
        "            return -self.log_likelihood_z_cauchy(scale=std) + reg_Jxz*self.reg_Jxz_uniform()\n",
        "\n",
        "        if self.prior == 'normal':\n",
        "            if reg_Jxz == 0:\n",
        "                self.Txz.compile(optimizer, loss=loss_ML_normal)\n",
        "            else:\n",
        "                self.Txz.compile(optimizer, loss=loss_ML_normal_reg)\n",
        "        elif self.prior == 'lognormal':\n",
        "            if reg_Jxz == 0:\n",
        "                self.Txz.compile(optimizer, loss=loss_ML_lognormal)\n",
        "            else:\n",
        "                self.Txz.compile(optimizer, loss=loss_ML_lognormal_reg)\n",
        "        elif self.prior == 'cauchy':\n",
        "            if reg_Jxz == 0:\n",
        "                self.Txz.compile(optimizer, loss=loss_ML_cauchy)\n",
        "            else:\n",
        "                self.Txz.compile(optimizer, loss=loss_ML_cauchy_reg)\n",
        "        else:\n",
        "            raise NotImplementedError('ML for prior ' + self.prior + ' is not implemented.')\n",
        "\n",
        "        if xval is not None:\n",
        "            validation_data = (xval, np.zeros_like(xval))\n",
        "        else:\n",
        "            validation_data = None\n",
        "\n",
        "        #hist = self.Txz.fit(x=x, y=np.zeros_like(x), validation_data=validation_data,\n",
        "        #                    batch_size=batch_size, epochs=epochs, verbose=verbose, shuffle=True)\n",
        "        # data preprocessing\n",
        "        N = x.shape[0]\n",
        "        I = np.arange(N)\n",
        "        loss_train = []\n",
        "        energies_x_val = []\n",
        "        energies_z_val = []\n",
        "        loss_val = []\n",
        "        y = np.zeros((batch_size, self.dim))\n",
        "        for e in range(epochs):\n",
        "            # sample batch\n",
        "            x_batch = x[np.random.choice(I, size=batch_size, replace=True)]\n",
        "            l = self.Txz.train_on_batch(x=x_batch, y=y)\n",
        "            loss_train.append(l)\n",
        "\n",
        "            # validate\n",
        "            if xval is not None:\n",
        "                xval_batch = xval[np.random.choice(I, size=batch_size, replace=True)]\n",
        "                l = self.Txz.test_on_batch(x=xval_batch, y=y)\n",
        "                loss_val.append(l)\n",
        "                if return_test_energies:\n",
        "                    z = self.sample_z(nsample=batch_size)\n",
        "                    xout = self.transform_zx(z)\n",
        "                    energies_x_val.append(self.energy_model.energy(xout))\n",
        "                    zout = self.transform_xz(xval_batch)\n",
        "                    energies_z_val.append(self.energy_z(zout))\n",
        "\n",
        "            # print\n",
        "            if verbose > 0:\n",
        "                str_ = 'Epoch ' + str(e) + '/' + str(epochs) + ' '\n",
        "                str_ += self.Txz.metrics_names[0] + ' '\n",
        "                str_ += '{:.4f}'.format(loss_train[-1]) + ' '\n",
        "                if xval is not None:\n",
        "                    str_ += '{:.4f}'.format(loss_val[-1]) + ' '\n",
        "#                for i in range(len(self.Txz.metrics_names)):\n",
        "\n",
        "                    #str_ += self.Txz.metrics_names[i] + ' '\n",
        "                    #str_ += '{:.4f}'.format(loss_train[-1][i]) + ' '\n",
        "                    #if xval is not None:\n",
        "                    #    str_ += '{:.4f}'.format(loss_val[-1][i]) + ' '\n",
        "                print(str_)\n",
        "                sys.stdout.flush()\n",
        "\n",
        "        if return_test_energies:\n",
        "            return loss_train, loss_val, energies_x_val, energies_z_val\n",
        "        else:\n",
        "            return loss_train, loss_val\n",
        "\n",
        "    def transform_xz(self, x):\n",
        "        return self.Txz.predict(ensure_traj(x))\n",
        "\n",
        "    def transform_xzJ(self, x):\n",
        "        x = ensure_traj(x)\n",
        "        if self.TxzJ is None:\n",
        "            return self.Txz.predict(x), np.zeros(x.shape[0])\n",
        "        else:\n",
        "            z, J = self.TxzJ.predict(x)\n",
        "            return z, J[:, 0]\n",
        "\n",
        "    def transform_zx(self, z):\n",
        "        return self.Tzx.predict(ensure_traj(z))\n",
        "\n",
        "    def transform_zxJ(self, z):\n",
        "        z = ensure_traj(z)\n",
        "        if self.TxzJ is None:\n",
        "            return self.Tzx.predict(z), np.zeros(z.shape[0])\n",
        "        else:\n",
        "            x, J = self.TzxJ.predict(z)\n",
        "            return x, J[:, 0]\n",
        "\n",
        "    def std_z(self, x):\n",
        "        \"\"\" Computes average standard deviation from the origin in z for given x \"\"\"\n",
        "        z = self.Txz.predict(x)\n",
        "        sigma = np.mean(z**2, axis=0)\n",
        "        z_std_ = np.sqrt(np.mean(sigma))\n",
        "        return z_std_\n",
        "\n",
        "    def energy_z(self, z, temperature=1.0):\n",
        "        if self.prior == 'normal':\n",
        "            E = self.dim * np.log(np.sqrt(temperature)) + np.sum(z**2 / (2*temperature), axis=1)\n",
        "        elif self.prior == 'lognormal':\n",
        "            sample_z_normal = np.log(z)\n",
        "            E = np.sum(sample_z_normal**2 / (2*temperature), axis=1) + np.sum(sample_z_normal, axis=1)\n",
        "        elif self.prior == 'cauchy':\n",
        "            E = np.sum(np.log(1 + (z/temperature)**2), axis=1)\n",
        "        return E\n",
        "\n",
        "    def sample_z(self, temperature=1.0, nsample=100000, return_energy=False):\n",
        "        \"\"\" Samples from prior distribution in x and produces generated x configurations\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        temperature : float\n",
        "            Relative temperature. Equal to the variance of the isotropic Gaussian sampled in z-space.\n",
        "        nsample : int\n",
        "            Number of samples\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        sample_z : array\n",
        "            Samples in z-space\n",
        "        energy_z : array\n",
        "            Energies of z samples (optional)\n",
        "\n",
        "        \"\"\"\n",
        "        sample_z = None\n",
        "        energy_z = None\n",
        "        if self.prior == 'normal':\n",
        "            sample_z = np.sqrt(temperature) * np.random.randn(nsample, self.dim)\n",
        "        elif self.prior == 'lognormal':\n",
        "            sample_z_normal = np.sqrt(temperature) * np.random.randn(nsample, self.dim)\n",
        "            sample_z = np.exp(sample_z_normal)\n",
        "        elif self.prior == 'cauchy':\n",
        "            from scipy.stats import cauchy\n",
        "            sample_z = cauchy(loc=0, scale=temperature).rvs(size=(nsample, self.dim))\n",
        "        else:\n",
        "            raise NotImplementedError('Sampling for prior ' + self.prior + ' is not implemented.')\n",
        "\n",
        "        if return_energy:\n",
        "            E = self.energy_z(sample_z)\n",
        "            return sample_z, E\n",
        "        else:\n",
        "            return sample_z\n",
        "\n",
        "class EnergyInvNet(InvNet):\n",
        "\n",
        "    def __init__(self, energy_model, layers, prior='normal'):\n",
        "        \"\"\" Invertible net where we have an energy function that defines p(x) \"\"\"\n",
        "        self.energy_model = energy_model\n",
        "        super().__init__(energy_model.dim, layers, prior=prior)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, filename, energy_model, clear_session=True):\n",
        "        \"\"\" Loads parameters into model. Careful: this clears the whole TF session!!\n",
        "        \"\"\"\n",
        "        from deep_boltzmann.util import load_obj\n",
        "        if clear_session:\n",
        "                keras.backend.clear_session()\n",
        "        D = load_obj(filename)\n",
        "        prior = D['prior']\n",
        "        layerdicts = D['layers']\n",
        "        layers = [eval(d['type']).from_dict(d) for d in layerdicts]\n",
        "        return EnergyInvNet(energy_model, layers, prior=prior)\n",
        "\n",
        "    # TODO: This is only implemented for the normal prior.\n",
        "    def log_w(self, high_energy, max_energy, temperature_factors=1.0):\n",
        "        \"\"\" Computes the variance of the log reweighting factors\n",
        "        \"\"\"\n",
        "        from deep_boltzmann.util import linlogcut\n",
        "        z = self.input_z\n",
        "        x = self.output_x\n",
        "        # compute z energy\n",
        "        Ez = self.dim * tf.math.log(tf.sqrt(temperature_factors)) + tf.reduce_sum(input_tensor=z**2, axis=1) / (2.0 * temperature_factors)\n",
        "        # compute x energy and regularize\n",
        "        Ex = self.energy_model.energy_tf(x) / temperature_factors\n",
        "        Exreg = linlogcut(Ex, high_energy, max_energy, tf=True)\n",
        "        # log weight\n",
        "        log_w = -Exreg + Ez + self.log_det_Jzx[:, 0]\n",
        "        return log_w\n",
        "\n",
        "    def sample(self, temperature=1.0, nsample=100000):\n",
        "        \"\"\" Samples from prior distribution in x and produces generated x configurations\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        temperature : float\n",
        "            Relative temperature. Equal to the variance of the isotropic Gaussian sampled in z-space.\n",
        "        nsample : int\n",
        "            Number of samples\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        sample_z : array\n",
        "            Samples in z-space\n",
        "        sample_x : array\n",
        "            Samples in x-space\n",
        "        energy_z : array\n",
        "            Energies of z samples\n",
        "        energy_x : array\n",
        "            Energies of x samples\n",
        "        log_w : array\n",
        "            Log weight of samples\n",
        "\n",
        "        \"\"\"\n",
        "        sample_z, energy_z = self.sample_z(temperature=temperature, nsample=nsample, return_energy=True)\n",
        "        sample_x, Jzx = self.transform_zxJ(sample_z)\n",
        "        energy_x = self.energy_model.energy(sample_x) / temperature\n",
        "        logw = -energy_x + energy_z + Jzx\n",
        "\n",
        "        return sample_z, sample_x, energy_z, energy_x, logw\n",
        "\n",
        "    def log_KL_x(self, high_energy, max_energy, temperature_factors=1.0, explore=1.0):\n",
        "        \"\"\" Computes the KL divergence with respect to z|x and the Boltzmann distribution\n",
        "        \"\"\"\n",
        "        from deep_boltzmann.util import linlogcut, _clip_high_tf, _linlogcut_tf_constantclip\n",
        "        x = self.output_x\n",
        "        # compute energy\n",
        "        E = self.energy_model.energy_tf(x) / temperature_factors\n",
        "        # regularize using log\n",
        "        Ereg = linlogcut(E, high_energy, max_energy, tf=True)\n",
        "        #Ereg = _linlogcut_tf_constantclip(E, high_energy, max_energy)\n",
        "        # gradient_clip(bg1.energy_model.energy_tf, 1e16, 1e20)\n",
        "        #return self.log_det_Jzx + Ereg\n",
        "        return -explore * self.log_det_Jzx[:, 0] + Ereg\n",
        "\n",
        "    def log_GaussianPriorMCMC_efficiency(self, high_energy, max_energy, metric=None, symmetric=False):\n",
        "        \"\"\" Computes the efficiency of GaussianPriorMCMC from a parallel x1->z1, z2->x2 network.\n",
        "\n",
        "        If metric is given, computes the efficiency as distance + log p_acc, where distance\n",
        "        is computed by |x1-x2|**2\n",
        "\n",
        "        \"\"\"\n",
        "        from deep_boltzmann.util import linlogcut\n",
        "        # define variables\n",
        "        x1 = self.input_x\n",
        "        x2 = self.output_x\n",
        "        z1 = self.output_z\n",
        "        z2 = self.input_z\n",
        "        # prior entropies\n",
        "        H1 = 0.5 * tf.reduce_sum(input_tensor=z1**2, axis=1)\n",
        "        H2 = 0.5 * tf.reduce_sum(input_tensor=z2**2, axis=1)\n",
        "        # compute and regularize energies\n",
        "        E1 = self.energy_model.energy_tf(x1)\n",
        "        E1reg = linlogcut(E1, high_energy, max_energy, tf=True)\n",
        "        E2 = self.energy_model.energy_tf(x2)\n",
        "        E2reg = linlogcut(E2, high_energy, max_energy, tf=True)\n",
        "        # free energy of samples\n",
        "        F1 = E1reg - H1 + self.log_det_xz[:, 0]\n",
        "        F2 = E2reg - H2 - self.log_det_zx[:, 0]\n",
        "        # acceptance probability\n",
        "        if symmetric:\n",
        "            arg1 = linlogcut(F2 - F1, 10, 1000, tf=True)\n",
        "            arg2 = linlogcut(F1 - F2, 10, 1000, tf=True)\n",
        "            log_pacc = -tf.math.log(1 + tf.exp(arg1)) - tf.math.log(1 + tf.exp(arg2))\n",
        "        else:\n",
        "            arg = linlogcut(F2 - F1, 10, 1000, tf=True)\n",
        "            log_pacc = -tf.math.log(1 + tf.exp(arg))\n",
        "        # mean square distance\n",
        "        if metric is None:\n",
        "            return log_pacc\n",
        "        else:\n",
        "            d = (metric(x1) - metric(x2)) ** 2\n",
        "            return d + log_pacc\n",
        "\n",
        "    def log_GaussianPriorMCMC_efficiency_unsupervised(self, high_energy, max_energy, metric=None):\n",
        "        \"\"\" Computes the efficiency of GaussianPriorMCMC\n",
        "        \"\"\"\n",
        "        from deep_boltzmann.util import linlogcut\n",
        "        # prior entropy\n",
        "        z = self.input_z\n",
        "        H = 0.5 * tf.reduce_sum(input_tensor=z**2, axis=1)\n",
        "        # compute and regularize energy\n",
        "        x = self.output_x\n",
        "        E = self.energy_model.energy_tf(x)\n",
        "        J = self.log_det_Jzx[:, 0]\n",
        "        Ereg = linlogcut(E, high_energy, max_energy, tf=True)\n",
        "        # free energy of samples\n",
        "        F = Ereg - H - J\n",
        "        # acceptance probability\n",
        "        arg = linlogcut(F[1:] - F[:-1], 10, 1000, tf=True)\n",
        "        log_pacc = -tf.math.log(1 + tf.exp(arg))\n",
        "        # mean square distance\n",
        "        # log_dist2 = tf.log(tf.reduce_mean((x[1:] - x[:-1])**2, axis=1))\n",
        "        # complement with 0's\n",
        "        log_pacc_0_ = tf.concat([np.array([0], dtype=np.float32), log_pacc], 0)\n",
        "        log_pacc__0 = tf.concat([log_pacc, np.array([0], dtype=np.float32)], 0)\n",
        "        if metric is None:\n",
        "            return log_pacc_0_ + log_pacc__0\n",
        "        else:\n",
        "            d = (metric(x)[1:] - metric(x)[:1]) ** 2\n",
        "            d_0_ = tf.concat([np.array([0], dtype=np.float32), d], 0)\n",
        "            d__0 = tf.concat([d, np.array([0], dtype=np.float32)], 0)\n",
        "            return d_0_ + d__0 + log_pacc_0_ + log_pacc__0\n",
        "\n",
        "    def train_KL(self, optimizer=None, lr=0.001, epochs=2000, batch_size=1024, verbose=1, clipnorm=None,\n",
        "                 high_energy=100, max_energy=1e10, temperature=1.0, explore=1.0):\n",
        "        if optimizer is None:\n",
        "            if clipnorm is None:\n",
        "                optimizer = keras.optimizers.adam(lr=lr)\n",
        "            else:\n",
        "                optimizer = keras.optimizers.adam(lr=lr, clipnorm=clipnorm)\n",
        "\n",
        "        import numbers\n",
        "        if isinstance(temperature, numbers.Number):\n",
        "            temperature = np.array([temperature])\n",
        "        else:\n",
        "            temperature = np.array(temperature)\n",
        "        tfac = np.tile(temperature, int(batch_size / temperature.size) + 1)[:batch_size]\n",
        "\n",
        "        def loss_KL(y_true, y_pred):\n",
        "            return self.log_KL_x(high_energy, max_energy, temperature_factors=tfac, explore=explore)\n",
        "\n",
        "        self.Tzx.compile(optimizer, loss=loss_KL)\n",
        "\n",
        "        dummy_output = np.zeros((batch_size, self.dim))\n",
        "        train_loss = []\n",
        "        for e in range(epochs):\n",
        "            # train in batches\n",
        "            #w = np.sqrt(tfac)[:, None] * np.random.randn(batch_size, self.dim)\n",
        "            w = self.sample_z(temperature=tfac[:, None], nsample=batch_size, return_energy=False)\n",
        "            # w = np.random.randn(batch_size, self.dim)\n",
        "            train_loss_batch = self.Tzx.train_on_batch(x=w, y=dummy_output)\n",
        "            train_loss.append(train_loss_batch)\n",
        "            if verbose == 1:\n",
        "                print('Epoch', e, ' loss', np.mean(train_loss_batch))\n",
        "                sys.stdout.flush()\n",
        "        train_loss = np.array(train_loss)\n",
        "\n",
        "        return train_loss\n",
        "\n",
        "    def train_flexible(self, x, xval=None, optimizer=None, lr=0.001, epochs=2000, batch_size=1024, verbose=1,\n",
        "                       clipnorm=None, high_energy=100, max_energy=1e10, std=1.0, reg_Jxz=0.0,\n",
        "                       weight_ML=1.0,\n",
        "                       weight_KL=1.0, temperature=1.0, explore=1.0,\n",
        "                       weight_MC=0.0, metric=None, symmetric_MC=False, supervised_MC=True,\n",
        "                       weight_W2=0.0,\n",
        "                       weight_RCEnt=0.0, rc_func=None, rc_min=0.0, rc_max=1.0,\n",
        "                       return_test_energies=False):\n",
        "        import numbers\n",
        "        if isinstance(temperature, numbers.Number):\n",
        "            temperature = np.array([temperature])\n",
        "        else:\n",
        "            temperature = np.array(temperature)\n",
        "        temperature = temperature.astype(np.float32)\n",
        "        # redefine batch size to be a multiple of temperatures\n",
        "        batch_size_per_temp = int(batch_size / temperature.size)\n",
        "        batch_size = int(temperature.size * batch_size_per_temp)\n",
        "        tidx = np.tile(np.arange(temperature.size), batch_size_per_temp)\n",
        "        tfac = temperature[tidx]\n",
        "\n",
        "        # Assemble Loss function\n",
        "        def loss_ML(y_true, y_pred):\n",
        "            return -self.log_likelihood_z_normal(std=std)\n",
        "        def loss_ML_reg(y_true, y_pred):\n",
        "            return -self.log_likelihood_z_normal(std=std) + reg_Jxz*self.reg_Jxz_uniform()\n",
        "        def loss_KL(y_true, y_pred):\n",
        "            return self.log_KL_x(high_energy, max_energy, temperature_factors=tfac, explore=explore)\n",
        "        def loss_MCEff_supervised(y_true, y_pred):\n",
        "            return -self.log_GaussianPriorMCMC_efficiency(high_energy, max_energy, metric=metric, symmetric=symmetric_MC)\n",
        "        def loss_MCEff_unsupervised(y_true, y_pred):\n",
        "            return -self.log_GaussianPriorMCMC_efficiency_unsupervised(high_energy, max_energy, metric=metric)\n",
        "        def loss_MCEff_combined(y_true, y_pred):\n",
        "            return -self.log_GaussianPriorMCMC_efficiency(high_energy, max_energy, metric=metric, symmetric=symmetric_MC) \\\n",
        "                   -0.5 * self.log_GaussianPriorMCMC_efficiency_unsupervised(high_energy, max_energy, metric=metric)\n",
        "        def loss_W2_var(y_true, y_pred):\n",
        "            # compute all reweighting factors\n",
        "            lw = self.log_w(high_energy, max_energy, temperature_factors=tfac)\n",
        "            # reshape to a column per temperature\n",
        "            lwT = tf.reshape(lw, (batch_size_per_temp, temperature.size))\n",
        "            lwT_mean = tf.reduce_mean(input_tensor=lwT, axis=0, keepdims=True)\n",
        "            return tf.reduce_mean(input_tensor=(lwT - lwT_mean) ** 2)\n",
        "        def loss_W2_dev(y_true, y_pred):\n",
        "            # compute all reweighting factors\n",
        "            lw = self.log_w(high_energy, max_energy, temperature_factors=tfac)\n",
        "            # reshape to a column per temperature\n",
        "            lwT = tf.reshape(lw, (batch_size_per_temp, temperature.size))\n",
        "            lwT_mean = tf.reduce_mean(input_tensor=lwT, axis=0, keepdims=True)\n",
        "            return tf.reduce_mean(input_tensor=tf.abs(lwT - lwT_mean))\n",
        "        gmeans = None\n",
        "        gstd = 0.0\n",
        "        if weight_RCEnt > 0.0:\n",
        "            gmeans = np.linspace(rc_min, rc_max, 11)\n",
        "            gstd = (rc_max - rc_min) / 11.0\n",
        "        def loss_RCEnt(y_true, y_pred):\n",
        "            return -self.rc_entropy(rc_func, gmeans, gstd, temperature.size)\n",
        "        inputs = []\n",
        "        outputs = []\n",
        "        losses = []\n",
        "        loss_weights = []\n",
        "        if weight_ML > 0:\n",
        "            inputs.append(self.input_x)\n",
        "            outputs.append(self.output_z)\n",
        "            if reg_Jxz == 0:\n",
        "                losses.append(loss_ML)\n",
        "            else:\n",
        "                losses.append(loss_ML_reg)\n",
        "            loss_weights.append(weight_ML)\n",
        "        if weight_KL > 0:\n",
        "            inputs.append(self.input_z)\n",
        "            outputs.append(self.output_x)\n",
        "            losses.append(loss_KL)\n",
        "            loss_weights.append(weight_KL)\n",
        "        if weight_MC > 0:\n",
        "            if self.input_z not in inputs:\n",
        "                inputs.append(self.input_z)\n",
        "            #if self.output_x not in outputs:\n",
        "            outputs.append(self.output_x)\n",
        "            if supervised_MC == 'both':\n",
        "                losses.append(loss_MCEff_combined)\n",
        "            elif supervised_MC is True:\n",
        "                losses.append(loss_MCEff_supervised)\n",
        "            else:\n",
        "                losses.append(loss_MCEff_unsupervised)\n",
        "            loss_weights.append(weight_MC)\n",
        "        if weight_W2 > 0:\n",
        "            if self.input_z not in inputs:\n",
        "                inputs.append(self.input_z)\n",
        "            #if self.output_x not in outputs:\n",
        "            outputs.append(self.output_x)\n",
        "            losses.append(loss_W2_dev)\n",
        "            loss_weights.append(weight_W2)\n",
        "        if weight_RCEnt > 0:\n",
        "            if self.input_z not in inputs:\n",
        "                inputs.append(self.input_z)\n",
        "            #if self.output_x not in outputs:\n",
        "            outputs.append(self.output_x)\n",
        "            losses.append(loss_RCEnt)\n",
        "            loss_weights.append(weight_RCEnt)\n",
        "\n",
        "        # data preprocessing\n",
        "        N = x.shape[0]\n",
        "        I = np.arange(N)\n",
        "        if xval is not None:\n",
        "            Nval = xval.shape[0]\n",
        "            Ival = np.arange(N)\n",
        "        else:\n",
        "            Nval = 0\n",
        "            Ival = None\n",
        "\n",
        "        # build estimator\n",
        "        if optimizer is None:\n",
        "            if clipnorm is None:\n",
        "                optimizer = keras.optimizers.adam(lr=lr)\n",
        "            else:\n",
        "                optimizer = keras.optimizers.adam(lr=lr, clipnorm=clipnorm)\n",
        "\n",
        "        # assemble model\n",
        "        dual_model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "        dual_model.compile(optimizer=optimizer, loss=losses, loss_weights=loss_weights)\n",
        "\n",
        "        # training loop\n",
        "        dummy_output = np.zeros((batch_size, self.dim))\n",
        "        y = [dummy_output for o in outputs]\n",
        "        loss_train = []\n",
        "        energies_x_val = []\n",
        "        energies_z_val = []\n",
        "        loss_val = []\n",
        "        for e in range(epochs):\n",
        "            # sample batch\n",
        "            x_batch = x[np.random.choice(I, size=batch_size, replace=True)]\n",
        "            w_batch = np.sqrt(tfac)[:, None] * np.random.randn(batch_size, self.dim)\n",
        "            l = dual_model.train_on_batch(x=[x_batch, w_batch], y=y)\n",
        "            loss_train.append(l)\n",
        "\n",
        "            # validate\n",
        "            if xval is not None:\n",
        "                xval_batch = xval[np.random.choice(I, size=batch_size, replace=True)]\n",
        "                wval_batch = np.sqrt(tfac)[:, None] * np.random.randn(batch_size, self.dim)\n",
        "                l = dual_model.test_on_batch(x=[xval_batch, wval_batch], y=y)\n",
        "                loss_val.append(l)\n",
        "                if return_test_energies:\n",
        "                    xout = self.transform_zx(wval_batch)\n",
        "                    energies_x_val.append(self.energy_model.energy(xout))\n",
        "                    zout = self.transform_xz(xval_batch)\n",
        "                    energies_z_val.append(self.energy_z(zout))\n",
        "\n",
        "            # print\n",
        "            if verbose > 0:\n",
        "                str_ = 'Epoch ' + str(e) + '/' + str(epochs) + ' '\n",
        "                for i in range(len(dual_model.metrics_names)):\n",
        "                    str_ += dual_model.metrics_names[i] + ' '\n",
        "                    str_ += '{:.4f}'.format(loss_train[-1][i]) + ' '\n",
        "                    if xval is not None:\n",
        "                        str_ += '{:.4f}'.format(loss_val[-1][i]) + ' '\n",
        "                print(str_)\n",
        "                sys.stdout.flush()\n",
        "\n",
        "        if return_test_energies:\n",
        "            return dual_model.metrics_names, np.array(loss_train), np.array(loss_val), energies_x_val, energies_z_val\n",
        "        else:\n",
        "            return dual_model.metrics_names, np.array(loss_train), np.array(loss_val)\n",
        "\n",
        "def invnet(dim, layer_types, energy_model=None, channels=None,\n",
        "           nl_layers=2, nl_hidden=100, nl_layers_scale=None, nl_hidden_scale=None,\n",
        "           nl_activation='relu', nl_activation_scale='tanh', scale=None, prior='normal',\n",
        "           permute_atomwise=False, permute_order=None,\n",
        "           whiten=None, whiten_keepdims=None,\n",
        "           ic=None, ic_cart=None, ic_norm=None, ic_cart_norm=None, torsion_cut=None, ic_jacobian_regularizer=1e-10,\n",
        "           rg_splitfrac=0.5,\n",
        "           **layer_args):\n",
        "    \"\"\"\n",
        "    layer_types : str\n",
        "        String describing the sequence of layers. Usage:\n",
        "            N NICER layer\n",
        "            n NICER layer, share parameters with last layer\n",
        "            R RealNVP layer\n",
        "            r RealNVP layer, share parameters with last layer\n",
        "            S Scaling layer\n",
        "            W Whiten layer\n",
        "            P Permute layer\n",
        "            Z Split dimensions off to latent space, leads to a merge and 3-way split.\n",
        "        Splitting and merging layers will be added automatically\n",
        "    energy_model : Energy model class\n",
        "        Class with energy() and dim\n",
        "    channels : array or None\n",
        "        Assignment of dimensions to channels (0/1 array of length ndim)\n",
        "    nl_layers : int\n",
        "        Number of hidden layers in the nonlinear transformations\n",
        "    nl_hidden : int\n",
        "        Number of hidden units in each nonlinear layer\n",
        "    nl_activation : str\n",
        "        Hidden-neuron activation functions used in the nonlinear layers\n",
        "    nl_activation_scale : str\n",
        "        Hidden-neuron activation functions used in scaling networks. If None, nl_activation will be used.\n",
        "    scale : None or float\n",
        "        If a scaling layer is used, fix the scale to this number. If None, scaling layers are trainable\n",
        "    prior : str\n",
        "        Form of the prior distribution\n",
        "    whiten : None or array\n",
        "        If not None, compute a whitening transformation with respect togiven coordinates\n",
        "    whiten_keepdims : None or int\n",
        "        Number of largest-variance dimensions to keep after whitening.\n",
        "    ic : None or array\n",
        "        If not None, compute internal coordinates using this Z index matrix. Do not mix with whitening.\n",
        "    ic_cart : None or array\n",
        "        If not None, use cartesian coordinates and whitening for these atoms.\n",
        "    ic_norm : None or array\n",
        "        If not None, these x coordinates will be used to compute the IC mean and std. These will be used\n",
        "        for normalization\n",
        "    torsion_cut : None or aray\n",
        "        If given defines where the torsions are cut\n",
        "    rg_splitfrac : float\n",
        "        Splitting fraction for Z layers\n",
        "\n",
        "    \"\"\"\n",
        "    # fix channels\n",
        "    channels, indices_split, indices_merge = split_merge_indices(dim, nchannels=2, channels=channels)\n",
        "\n",
        "    # augment layer types with split and merge layers\n",
        "    split = False\n",
        "    tmp = ''\n",
        "    if whiten is not None:\n",
        "        tmp += 'W'\n",
        "    if ic is not None:\n",
        "        tmp += 'I'\n",
        "    for ltype in layer_types:\n",
        "        if (ltype == 'S' or ltype == 'P') and split:\n",
        "            tmp += '>'\n",
        "            split = False\n",
        "        if (ltype == 'N' or ltype == 'R') and not split:\n",
        "            tmp += '<'\n",
        "            split = True\n",
        "        tmp += ltype\n",
        "    if split:\n",
        "        tmp += '>'\n",
        "    layer_types = tmp\n",
        "    print(layer_types)\n",
        "\n",
        "    # prepare layers\n",
        "    layers = []\n",
        "\n",
        "    if nl_activation_scale is None:\n",
        "        nl_activation_scale = nl_activation\n",
        "    if nl_layers_scale is None:\n",
        "        nl_layers_scale = nl_layers\n",
        "    if nl_hidden_scale is None:\n",
        "        nl_hidden_scale= nl_hidden\n",
        "\n",
        "    # number of dimensions left in the signal. The remaining dimensions are going to latent space directly\n",
        "    dim_L = dim\n",
        "    dim_R = 0\n",
        "    dim_Z = 0\n",
        "\n",
        "    # translate and scale layers\n",
        "    T1 = None\n",
        "    T2 = None\n",
        "    S1 = None\n",
        "    S2 = None\n",
        "\n",
        "    for ltype in layer_types:\n",
        "        print(ltype, dim_L, dim_R, dim_Z)\n",
        "        if ltype == '<':\n",
        "            if dim_R > 0:\n",
        "                raise RuntimeError('Already split. Cannot invoke split layer.')\n",
        "            channels_cur = np.concatenate([channels[:dim_L], np.tile([2], dim_Z)])\n",
        "            dim_L = np.count_nonzero(channels_cur==0)\n",
        "            dim_R = np.count_nonzero(channels_cur==1)\n",
        "            layers.append(SplitChannels(dim, channels=channels_cur))\n",
        "        elif ltype == '>':\n",
        "            if dim_R == 0:\n",
        "                raise RuntimeError('Not split. Cannot invoke merge layer.')\n",
        "            channels_cur = np.concatenate([channels[:(dim_L+dim_R)], np.tile([2], dim_Z)])\n",
        "            dim_L += dim_R\n",
        "            dim_R = 0\n",
        "            layers.append(MergeChannels(dim, channels=channels_cur))\n",
        "        elif ltype == 'P':\n",
        "            if permute_atomwise:\n",
        "                order_atomwise = np.arange(dim).reshape((dim//3, 3))\n",
        "                permut_ = np.random.choice(dim//3, dim//3, replace=False)\n",
        "                layers.append(Permute(dim, order=order_atomwise[permut_, :].flatten() ))\n",
        "            else:\n",
        "                if dim_Z > 0 and permute_order is None:\n",
        "                    order = np.concatenate([np.random.choice(dim-dim_Z, dim-dim_Z, replace=False),\n",
        "                                            np.arange(dim-dim_Z, dim)])\n",
        "                layers.append(Permute(dim, order=permute_order))\n",
        "        elif ltype == 'N':\n",
        "            if dim_R == 0:\n",
        "                raise RuntimeError('Not split. Cannot invoke NICE layer.')\n",
        "            T1 = nonlinear_transform(dim_R, nlayers=nl_layers, nhidden=nl_hidden,\n",
        "                                     activation=nl_activation, **layer_args)\n",
        "            T2 = nonlinear_transform(dim_L, nlayers=nl_layers, nhidden=nl_hidden,\n",
        "                                     activation=nl_activation, **layer_args)\n",
        "            layers.append(NICER([T1, T2]))\n",
        "        elif ltype == 'n':\n",
        "            if dim_R == 0:\n",
        "                raise RuntimeError('Not split. Cannot invoke NICE layer.')\n",
        "            layers.append(NICER([T1, T2]))\n",
        "        elif ltype == 'R':\n",
        "            if dim_R == 0:\n",
        "                raise RuntimeError('Not split. Cannot invoke RealNVP layer.')\n",
        "            S1 = nonlinear_transform(dim_R, nlayers=nl_layers_scale, nhidden=nl_hidden_scale,\n",
        "                                     activation=nl_activation_scale, init_outputs=0, **layer_args)\n",
        "            T1 = nonlinear_transform(dim_R, nlayers=nl_layers, nhidden=nl_hidden,\n",
        "                                     activation=nl_activation, **layer_args)\n",
        "            S2 = nonlinear_transform(dim_L, nlayers=nl_layers_scale, nhidden=nl_hidden_scale,\n",
        "                                     activation=nl_activation_scale, init_outputs=0, **layer_args)\n",
        "            T2 = nonlinear_transform(dim_L, nlayers=nl_layers, nhidden=nl_hidden,\n",
        "                                     activation=nl_activation, **layer_args)\n",
        "            layers.append(RealNVP([S1, T1, S2, T2]))\n",
        "        elif ltype == 'r':\n",
        "            if dim_R == 0:\n",
        "                raise RuntimeError('Not split. Cannot invoke RealNVP layer.')\n",
        "            layers.append(RealNVP([S1, T1, S2, T2]))\n",
        "        elif ltype == 'S':\n",
        "            if dim_R > 0:\n",
        "                raise RuntimeError('Not merged. Cannot invoke constant scaling layer.')\n",
        "            # scaling layer\n",
        "            if scale is None:\n",
        "                scaling_factors = None\n",
        "            else:\n",
        "                scaling_factors = scale * np.ones((1, dim))\n",
        "            layers.append(Scaling(dim, scaling_factors=scaling_factors, trainable=(scale is None)))\n",
        "        elif ltype == 'I':\n",
        "            if dim_R > 0:\n",
        "                raise RuntimeError('Already split. Cannot invoke IC layer.')\n",
        "            dim_L = dim_L - 6\n",
        "            dim_R = 0\n",
        "            dim_Z = 6\n",
        "            if ic_cart is None:\n",
        "                layer = InternalCoordinatesTransformation(ic, Xnorm=ic_norm, torsion_cut=torsion_cut)\n",
        "            else:\n",
        "                if ic_cart_norm is None:\n",
        "                    ic_cart_norm = ic_norm\n",
        "                layer = MixedCoordinatesTransformation(ic_cart, ic, X0=ic_cart_norm, X0ic=ic_norm, torsion_cut=torsion_cut, jacobian_regularizer=ic_jacobian_regularizer)\n",
        "            layers.append(layer)\n",
        "        elif ltype == 'W':\n",
        "            if dim_R > 0:\n",
        "                raise RuntimeError('Not merged. Cannot invoke whitening layer.')\n",
        "            W = FixedWhiten(whiten, keepdims=whiten_keepdims)\n",
        "            dim_L = W.keepdims\n",
        "            dim_Z = dim-W.keepdims\n",
        "            layers.append(W)\n",
        "        elif ltype == 'Z':\n",
        "            if dim_R == 0:\n",
        "                raise RuntimeError('Not split. Cannot invoke Z layer.')\n",
        "            if dim_L + dim_R <= 1:  # nothing left to split, so we ignore this layer and move on\n",
        "                break\n",
        "            # merge the current pattern\n",
        "            channels_cur = np.concatenate([channels[:(dim_L+dim_R)], np.tile([2], dim_Z)])\n",
        "            dim_L += dim_R\n",
        "            dim_R = 0\n",
        "            layers.append(MergeChannels(dim, channels=channels_cur))\n",
        "            # 3-way split\n",
        "            split_to_z = int((dim_L + dim_R) * rg_splitfrac)\n",
        "            split_to_z = max(1, split_to_z)  # split at least 1 dimension\n",
        "            dim_Z += split_to_z\n",
        "            dim_L -= split_to_z\n",
        "            channels_cur = np.concatenate([channels[:dim_L], np.tile([2], dim_Z)])\n",
        "            dim_L = np.count_nonzero(channels_cur==0)\n",
        "            dim_R = np.count_nonzero(channels_cur==1)\n",
        "            layers.append(SplitChannels(dim, channels=channels_cur))\n",
        "\n",
        "    if energy_model is None:\n",
        "        return InvNet(dim, layers, prior=prior)\n",
        "    else:\n",
        "        return EnergyInvNet(energy_model, layers, prior=prior)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJIhgobO-sc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "c3b2b2a0-6256-47db-f08d-229af263930a"
      },
      "source": [
        "network_NICER_KLML = invnet(double_well.dim, 'NNNNS', double_well, nl_layers=3, nl_hidden=100, \n",
        "                            nl_activation='relu', nl_activation_scale='tanh')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<NNNN>S\n",
            "< 2 0 0\n",
            "N 1 1 0\n",
            "N 1 1 0\n",
            "N 1 1 0\n",
            "N 1 1 0\n",
            "> 1 1 0\n",
            "S 2 0 0\n",
            "tracking <tf.Variable 'log_scale:0' shape=(1, 2) dtype=float32, numpy=array([[0., 0.]], dtype=float32)> log_scaling_factors\n",
            "tracking <tf.Variable 'log_scale:0' shape=(1, 2) dtype=float32, numpy=array([[0., 0.]], dtype=float32)> log_scaling_factors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb8-l9FYPMCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist_NICER_KLML1 = network_NICER_KLML.train_ML(x, xval=xval, epochs=1, batch_size=100, std=1.0, \n",
        "                                               verbose=0, return_test_energies=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6sFoz0ESWyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}